{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\meshari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from nexus_ai.sentence_sentiment_analysis import multi_class_model\n",
    "from nexus_ai.sentence_sentiment_analysis import model\n",
    "from nexus_ai.sentence_sentiment_analysis.sentence_sentiment_analysis_model import train, test, predict\n",
    "import numpy as np\n",
    "from nexus_ai.sentence_sentiment_analysis.preprocessing import clean_reviews, creat_vocab, tokenize_data, pad_features, train_test_split, google_clean_reviews, clean_reviews_and_creat_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  Positive  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...\n",
       "1  Positive  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...\n",
       "2  Positive  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...\n",
       "3  Positive  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...\n",
       "4  Positive  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/ar_reviews_100k.tsv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null valueslabel    0\n",
      "text     0\n",
      "dtype: int64\n",
      "number of duplicate values0\n"
     ]
    }
   ],
   "source": [
    "print(f'number of null values \\n{df.isnull().sum()}')\n",
    "print(f'number of duplicate values is {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels ['Positive' 'Mixed' 'Negative']\n"
     ]
    }
   ],
   "source": [
    "labels = df['label'].unique()\n",
    "print(f'labels {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's tranform Mixed into Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape before transforming: (99999, 2)\n",
      "dataset shape after transforming: (99999, 2)\n",
      "labels ['Positive' 'Neutral' 'Negative']\n"
     ]
    }
   ],
   "source": [
    "print(f'dataset shape before transforming: {df.shape}')\n",
    "df.loc[df[df['label']=='Mixed'].index, 'label'] = 'Neutral'\n",
    "# print(len((df[df['label']=='Mixed']).index.to_list()))\n",
    "# print(df.loc[df[(df['label']=='Mixed')].index, :])\n",
    "print(f'dataset shape after transforming: {df.shape}')\n",
    "labels = df['label'].unique()\n",
    "print(f'labels {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precentage 33333\n",
      "Negative precentage 33333\n",
      "Neutral precentage 33333\n"
     ]
    }
   ],
   "source": [
    "pos_prec = df[df['label'] == 'Positive'].shape[0]\n",
    "neg_prec = df[df['label'] == 'Negative'].shape[0]\n",
    "nat_prec = df[df['label'] == 'Neutral'].shape[0]\n",
    "print(f'Positive precentage {pos_prec}')\n",
    "print(f'Negative precentage {neg_prec}')\n",
    "print(f'Neutral precentage {nat_prec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset is balanced which is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's use the camel tools to preproccess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before unicode normalizing\n",
      "['ممتاز نوعا ما . النظافة والموقع والتجهيز والشاطيء. المطعم', 'أحد أسباب نجاح الإمارات أن كل شخص في هذه الدولة يعشق ترابها. نحن نحب الإمارات. ومضات من فكر. نصائح لدولة تطمح بالصفوف الأولى و قائد لا يقبل إلا براحة شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءة مرات كثيرة', 'هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى هدوء جبال الشيشان .. للتعرف على حقيقة ما يجرى فى تلك البلاد من حروب ضاربة بحق المسلمين و جزء كبير من تاريخ تلك المنطقة. التضحية .. الرجولة .. الوفاء والكثير من القيم الأخرى اثبتت وجودها فى تلك الرواية البسيطة', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطى مرحلة ان القارئ يخلص الرواية وهو فاتح بؤه لمرحلة ان القارئ يخلص الرواية وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبة ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطة مشاعر انسانية حقيقية لدرجة غريبة.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الآن ومصر من سنة هيطمنك ان عجينة الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمة دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمة من معنى..']\n",
      "after unicode normalizing\n",
      "['ممتاز نوعا ما . النظافة والموقع والتجهيز والشاطيء. المطعم', 'أحد أسباب نجاح الإمارات أن كل شخص في هذه الدولة يعشق ترابها. نحن نحب الإمارات. ومضات من فكر. نصائح لدولة تطمح بالصفوف الأولى و قائد لا يقبل إلا براحة شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءة مرات كثيرة', 'هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى هدوء جبال الشيشان .. للتعرف على حقيقة ما يجرى فى تلك البلاد من حروب ضاربة بحق المسلمين و جزء كبير من تاريخ تلك المنطقة. التضحية .. الرجولة .. الوفاء والكثير من القيم الأخرى اثبتت وجودها فى تلك الرواية البسيطة', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطى مرحلة ان القارئ يخلص الرواية وهو فاتح بؤه لمرحلة ان القارئ يخلص الرواية وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبة ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطة مشاعر انسانية حقيقية لدرجة غريبة.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الآن ومصر من سنة هيطمنك ان عجينة الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمة دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمة من معنى..']\n"
     ]
    }
   ],
   "source": [
    "print('before unicode normalizing')\n",
    "print(df.loc[:3, 'text'].to_list())\n",
    "df['text'] = df['text'].apply(lambda x: normalize_unicode(x))\n",
    "\n",
    "print('after unicode normalizing')\n",
    "print(df.loc[:3, 'text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oringinal text\n",
      "['ممتاز نوعا ما . النظافة والموقع والتجهيز والشاطيء. المطعم', 'أحد أسباب نجاح الإمارات أن كل شخص في هذه الدولة يعشق ترابها. نحن نحب الإمارات. ومضات من فكر. نصائح لدولة تطمح بالصفوف الأولى و قائد لا يقبل إلا براحة شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءة مرات كثيرة', 'هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى هدوء جبال الشيشان .. للتعرف على حقيقة ما يجرى فى تلك البلاد من حروب ضاربة بحق المسلمين و جزء كبير من تاريخ تلك المنطقة. التضحية .. الرجولة .. الوفاء والكثير من القيم الأخرى اثبتت وجودها فى تلك الرواية البسيطة', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطى مرحلة ان القارئ يخلص الرواية وهو فاتح بؤه لمرحلة ان القارئ يخلص الرواية وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبة ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطة مشاعر انسانية حقيقية لدرجة غريبة.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الآن ومصر من سنة هيطمنك ان عجينة الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمة دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمة من معنى..']\n",
      "text after Normalizing alef variants to (ا)\n",
      "['ممتاز نوعا ما . النظافة والموقع والتجهيز والشاطيء. المطعم', 'احد اسباب نجاح الامارات ان كل شخص في هذه الدولة يعشق ترابها. نحن نحب الامارات. ومضات من فكر. نصائح لدولة تطمح بالصفوف الاولى و قائد لا يقبل الا براحة شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءة مرات كثيرة', 'هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى هدوء جبال الشيشان .. للتعرف على حقيقة ما يجرى فى تلك البلاد من حروب ضاربة بحق المسلمين و جزء كبير من تاريخ تلك المنطقة. التضحية .. الرجولة .. الوفاء والكثير من القيم الاخرى اثبتت وجودها فى تلك الرواية البسيطة', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطى مرحلة ان القارئ يخلص الرواية وهو فاتح بؤه لمرحلة ان القارئ يخلص الرواية وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبة ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطة مشاعر انسانية حقيقية لدرجة غريبة.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الان ومصر من سنة هيطمنك ان عجينة الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمة دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمة من معنى..']\n",
      "text after Normalizing alef maksura (ى) to yeh (ي)\n",
      "['ممتاز نوعا ما . النظافة والموقع والتجهيز والشاطيء. المطعم', 'احد اسباب نجاح الامارات ان كل شخص في هذه الدولة يعشق ترابها. نحن نحب الامارات. ومضات من فكر. نصائح لدولة تطمح بالصفوف الاولي و قائد لا يقبل الا براحة شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءة مرات كثيرة', 'هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الي هدوء جبال الشيشان .. للتعرف علي حقيقة ما يجري في تلك البلاد من حروب ضاربة بحق المسلمين و جزء كبير من تاريخ تلك المنطقة. التضحية .. الرجولة .. الوفاء والكثير من القيم الاخري اثبتت وجودها في تلك الرواية البسيطة', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطي مرحلة ان القارئ يخلص الرواية وهو فاتح بؤه لمرحلة ان القارئ يخلص الرواية وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبة ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطة مشاعر انسانية حقيقية لدرجة غريبة.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الان ومصر من سنة هيطمنك ان عجينة الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمة دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمة من معني..']\n",
      "text after Normalizing teh marbuta (ة) to heh (ه)\n",
      "['ممتاز نوعا ما . النظافه والموقع والتجهيز والشاطيء. المطعم', 'احد اسباب نجاح الامارات ان كل شخص في هذه الدوله يعشق ترابها. نحن نحب الامارات. ومضات من فكر. نصائح لدوله تطمح بالصفوف الاولي و قائد لا يقبل الا براحه شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءه مرات كثيره', 'هادفه .. وقويه. تنقلك من صخب شوارع القاهره الي هدوء جبال الشيشان .. للتعرف علي حقيقه ما يجري في تلك البلاد من حروب ضاربه بحق المسلمين و جزء كبير من تاريخ تلك المنطقه. التضحيه .. الرجوله .. الوفاء والكثير من القيم الاخري اثبتت وجودها في تلك الروايه البسيطه', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطي مرحله ان القارئ يخلص الروايه وهو فاتح بؤه لمرحله ان القارئ يخلص الروايه وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبه ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطه مشاعر انسانيه حقيقيه لدرجه غريبه.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الان ومصر من سنه هيطمنك ان عجينه الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمه دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمه من معني..']\n"
     ]
    }
   ],
   "source": [
    "print('oringinal text')\n",
    "print(df.loc[:3, 'text'].to_list())\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: normalize_alef_ar(x))\n",
    "print('text after Normalizing alef variants to (ا)')\n",
    "print(df.loc[:3, 'text'].to_list())\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: normalize_alef_maksura_ar(x))\n",
    "print('text after Normalizing alef maksura (ى) to yeh (ي)')\n",
    "print(df.loc[:3, 'text'].to_list())\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: normalize_teh_marbuta_ar(x))\n",
    "print('text after Normalizing teh marbuta (ة) to heh (ه)')\n",
    "print(df.loc[:3, 'text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oringinal text\n",
      "['ممتاز نوعا ما . النظافه والموقع والتجهيز والشاطيء. المطعم', 'احد اسباب نجاح الامارات ان كل شخص في هذه الدوله يعشق ترابها. نحن نحب الامارات. ومضات من فكر. نصائح لدوله تطمح بالصفوف الاولي و قائد لا يقبل الا براحه شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءه مرات كثيره', 'هادفه .. وقويه. تنقلك من صخب شوارع القاهره الي هدوء جبال الشيشان .. للتعرف علي حقيقه ما يجري في تلك البلاد من حروب ضاربه بحق المسلمين و جزء كبير من تاريخ تلك المنطقه. التضحيه .. الرجوله .. الوفاء والكثير من القيم الاخري اثبتت وجودها في تلك الروايه البسيطه', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطي مرحله ان القارئ يخلص الروايه وهو فاتح بؤه لمرحله ان القارئ يخلص الروايه وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبه ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطه مشاعر انسانيه حقيقيه لدرجه غريبه.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الان ومصر من سنه هيطمنك ان عجينه الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمه دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمه من معني..']\n",
      "text after removing Arabic diacritical marks\n",
      "['ممتاز نوعا ما . النظافه والموقع والتجهيز والشاطيء. المطعم', 'احد اسباب نجاح الامارات ان كل شخص في هذه الدوله يعشق ترابها. نحن نحب الامارات. ومضات من فكر. نصائح لدوله تطمح بالصفوف الاولي و قائد لا يقبل الا براحه شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءه مرات كثيره', 'هادفه .. وقويه. تنقلك من صخب شوارع القاهره الي هدوء جبال الشيشان .. للتعرف علي حقيقه ما يجري في تلك البلاد من حروب ضاربه بحق المسلمين و جزء كبير من تاريخ تلك المنطقه. التضحيه .. الرجوله .. الوفاء والكثير من القيم الاخري اثبتت وجودها في تلك الروايه البسيطه', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطي مرحله ان القارئ يخلص الروايه وهو فاتح بؤه لمرحله ان القارئ يخلص الروايه وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبه ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطه مشاعر انسانيه حقيقيه لدرجه غريبه.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الان ومصر من سنه هيطمنك ان عجينه الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمه دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمه من معني..']\n"
     ]
    }
   ],
   "source": [
    "print('oringinal text')\n",
    "print(df.loc[:3, 'text'].to_list())\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: dediac_ar(x))\n",
    "print('text after removing Arabic diacritical marks')\n",
    "print(df.loc[:3, 'text'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sadly even in the sample original text there was no Arabic diacritical marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oringinal text\n",
      "['ممتاز نوعا ما . النظافه والموقع والتجهيز والشاطيء. المطعم', 'احد اسباب نجاح الامارات ان كل شخص في هذه الدوله يعشق ترابها. نحن نحب الامارات. ومضات من فكر. نصائح لدوله تطمح بالصفوف الاولي و قائد لا يقبل الا براحه شعبه وتوفر كل سب العيش الكريم. حكم و مواقف ونصائح لكل فرد فينا ليس بمجرد كتاب سياسي كما كنت اعتقد. يستحق القراءه مرات كثيره', 'هادفه .. وقويه. تنقلك من صخب شوارع القاهره الي هدوء جبال الشيشان .. للتعرف علي حقيقه ما يجري في تلك البلاد من حروب ضاربه بحق المسلمين و جزء كبير من تاريخ تلك المنطقه. التضحيه .. الرجوله .. الوفاء والكثير من القيم الاخري اثبتت وجودها في تلك الروايه البسيطه', 'خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الازرق ميقراش احسن.. احمد مراد تخطي مرحله ان القارئ يخلص الروايه وهو فاتح بؤه لمرحله ان القارئ يخلص الروايه وهو محترم الكاتب.. اتقان مخيف.. بصرف النظر عن اخطاء لا تذكر ف الحوار.. انما احمد مراد سافر عاش حبه ف اوائل القرن العشرين وجه ياخدنا لهناك.. خلطه مشاعر انسانيه حقيقيه لدرجه غريبه.. دراما نقلته من كاتب شاب بيستعرض لصنايعي حقيقي تثق فيما سيكتب بعد كده.. ربط بين مصر الان ومصر من سنه هيطمنك ان عجينه الناس دي متغيرتش و زي ما عدت قبل كده هتعدي اي ازمه دلوقتي.. احمد مراد.. كاتب محترف شغال بضميييررر بكل ما تحمل الكلمه من معني..']\n",
      "text after splitting\n",
      "[['ممتاز', 'نوعا', 'ما', '.', 'النظافه', 'والموقع', 'والتجهيز', 'والشاطيء', '.', 'المطعم'], ['احد', 'اسباب', 'نجاح', 'الامارات', 'ان', 'كل', 'شخص', 'في', 'هذه', 'الدوله', 'يعشق', 'ترابها', '.', 'نحن', 'نحب', 'الامارات', '.', 'ومضات', 'من', 'فكر', '.', 'نصائح', 'لدوله', 'تطمح', 'بالصفوف', 'الاولي', 'و', 'قائد', 'لا', 'يقبل', 'الا', 'براحه', 'شعبه', 'وتوفر', 'كل', 'سب', 'العيش', 'الكريم', '.', 'حكم', 'و', 'مواقف', 'ونصائح', 'لكل', 'فرد', 'فينا', 'ليس', 'بمجرد', 'كتاب', 'سياسي', 'كما', 'كنت', 'اعتقد', '.', 'يستحق', 'القراءه', 'مرات', 'كثيره'], ['هادفه', '.', '.', 'وقويه', '.', 'تنقلك', 'من', 'صخب', 'شوارع', 'القاهره', 'الي', 'هدوء', 'جبال', 'الشيشان', '.', '.', 'للتعرف', 'علي', 'حقيقه', 'ما', 'يجري', 'في', 'تلك', 'البلاد', 'من', 'حروب', 'ضاربه', 'بحق', 'المسلمين', 'و', 'جزء', 'كبير', 'من', 'تاريخ', 'تلك', 'المنطقه', '.', 'التضحيه', '.', '.', 'الرجوله', '.', '.', 'الوفاء', 'والكثير', 'من', 'القيم', 'الاخري', 'اثبتت', 'وجودها', 'في', 'تلك', 'الروايه', 'البسيطه'], ['خلصنا', '.', '.', 'مبدئيا', 'اللي', 'مستني', 'ابهار', 'زي', 'الفيل', 'الازرق', 'ميقراش', 'احسن', '.', '.', 'احمد', 'مراد', 'تخطي', 'مرحله', 'ان', 'القارئ', 'يخلص', 'الروايه', 'وهو', 'فاتح', 'بؤه', 'لمرحله', 'ان', 'القارئ', 'يخلص', 'الروايه', 'وهو', 'محترم', 'الكاتب', '.', '.', 'اتقان', 'مخيف', '.', '.', 'بصرف', 'النظر', 'عن', 'اخطاء', 'لا', 'تذكر', 'ف', 'الحوار', '.', '.', 'انما', 'احمد', 'مراد', 'سافر', 'عاش', 'حبه', 'ف', 'اوائل', 'القرن', 'العشرين', 'وجه', 'ياخدنا', 'لهناك', '.', '.', 'خلطه', 'مشاعر', 'انسانيه', 'حقيقيه', 'لدرجه', 'غريبه', '.', '.', 'دراما', 'نقلته', 'من', 'كاتب', 'شاب', 'بيستعرض', 'لصنايعي', 'حقيقي', 'تثق', 'فيما', 'سيكتب', 'بعد', 'كده', '.', '.', 'ربط', 'بين', 'مصر', 'الان', 'ومصر', 'من', 'سنه', 'هيطمنك', 'ان', 'عجينه', 'الناس', 'دي', 'متغيرتش', 'و', 'زي', 'ما', 'عدت', 'قبل', 'كده', 'هتعدي', 'اي', 'ازمه', 'دلوقتي', '.', '.', 'احمد', 'مراد', '.', '.', 'كاتب', 'محترف', 'شغال', 'بضميييررر', 'بكل', 'ما', 'تحمل', 'الكلمه', 'من', 'معني', '.', '.']]\n"
     ]
    }
   ],
   "source": [
    "print('oringinal text')\n",
    "print(df.loc[:3, 'text'].to_list())\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: simple_word_tokenize(x))\n",
    "print('text after splitting')\n",
    "print(df.loc[:3, 'text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the vocab is 35031\n"
     ]
    }
   ],
   "source": [
    "words = df['text'].tolist()\n",
    "words = [item for sublist in words for item in sublist]\n",
    "\n",
    "words_counter = Counter(words)\n",
    "words_set = sorted(Counter(words),key=Counter(words).get,reverse=True)\n",
    "vocab_to_int = []\n",
    "\n",
    "vocab_to_int = {word: index for index,word in enumerate(words_set, 1)  if words_counter[word] > 10}\n",
    "\n",
    "print(f'length of the vocab is {len(vocab_to_int)}')\n",
    "# vocab_to_int = {word: index for (index,word),stop in zip(enumerate(words_set, 1), range(50000))}\n",
    "# vocab_to_int = {word: index for index,word in enumerate(words_set, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', 1)\n",
      "('في', 2)\n",
      "('من', 3)\n",
      "('،', 4)\n",
      "('و', 5)\n",
      "('ان', 6)\n",
      "('علي', 7)\n",
      "('لا', 8)\n",
      "('ما', 9)\n",
      "('عن', 10)\n",
      "('جدا', 11)\n",
      "('لم', 12)\n",
      "('الي', 13)\n",
      "('الروايه', 14)\n",
      "('الكتاب', 15)\n",
      "('كل', 16)\n",
      "('كان', 17)\n",
      "('التي', 18)\n",
      "(':', 19)\n",
      "('هذا', 20)\n"
     ]
    }
   ],
   "source": [
    "#printing the highest 20 words mentioned in the data\n",
    "for i,stop in zip(vocab_to_int.items(), range(20)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's tokenize the text data to ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78, 320, 9, 1, 95, 1174, 0, 29330, 1, 289], [135, 1190, 2647, 1586, 6, 16, 265, 2, 30, 606, 6948, 0, 1, 381, 3053, 1586, 1, 15054, 3, 1311, 1, 3720, 18759, 32838, 0, 176, 5, 7646, 8, 4262, 36, 7389, 9467, 3991, 16, 12834, 2115, 1954, 1, 1157, 5, 187, 20050, 308, 2887, 1676, 70, 2465, 41, 2527, 58, 55, 208, 1, 221, 152, 796, 293], [10743, 1, 1, 14354, 1, 10914, 3, 10574, 5901, 1873, 13, 1573, 15055, 19379, 1, 1, 8595, 7, 401, 9, 4263, 2, 86, 1564, 3, 6792, 32839, 1337, 646, 5, 359, 198, 3, 433, 86, 1392, 1, 6659, 1, 1, 15056, 1, 1, 7647, 2204, 3, 2308, 426, 9323, 3489, 2, 86, 14, 1775]]\n"
     ]
    }
   ],
   "source": [
    "reviews = df['text'].tolist()\n",
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_ints.append([vocab_to_int.get(word, 0) for word in review])\n",
    "print(reviews_ints[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_labels class distribution:\n",
      "(0, 33333)\n",
      "(1, 33333)\n",
      "(2, 33333)\n"
     ]
    }
   ],
   "source": [
    "labels = df['label'].tolist()\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == 'Positive':\n",
    "        encoded_labels.append(2)\n",
    "    if label == 'Neutral':\n",
    "        encoded_labels.append(1)\n",
    "    elif label == 'Negative':\n",
    "        encoded_labels.append(0) \n",
    "\n",
    "unique, counts = np.unique(encoded_labels, return_counts=True)\n",
    "distribution = dict(zip(unique, counts))\n",
    "print('encoded_labels class distribution:')\n",
    "for item in distribution.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pad_features(reviews_ints, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999, 256)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make sure the classes (labels) are sorted to have a balanced splits\n"
     ]
    }
   ],
   "source": [
    "train_x, validate_x, test_x, train_y, validate_y, test_y = train_test_split(features, encoded_labels, numclasses=3, train_frac=0.8, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y class distribution:\n",
      "(0, 26666)\n",
      "(1, 26667)\n",
      "(2, 26667)\n",
      "validate_y class distribution:\n",
      "(0, 3333)\n",
      "(1, 3333)\n",
      "(2, 3333)\n",
      "test_y class distribution:\n",
      "(0, 3333)\n",
      "(1, 3333)\n",
      "(2, 3333)\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_y, return_counts=True)\n",
    "distribution = dict(zip(unique, counts))\n",
    "print('train_y class distribution:')\n",
    "for item in distribution.items():\n",
    "    print(item)\n",
    "\n",
    "unique, counts = np.unique(validate_y, return_counts=True)\n",
    "distribution = dict(zip(unique, counts))\n",
    "print('validate_y class distribution:')\n",
    "for item in distribution.items():\n",
    "    print(item)\n",
    "\n",
    "unique, counts = np.unique(test_y, return_counts=True)\n",
    "distribution = dict(zip(unique, counts))\n",
    "print('test_y class distribution:')\n",
    "for item in distribution.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 256) (80000,)\n",
      "(9999, 256) (9999,)\n",
      "(9999, 256) (9999,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(validate_x.shape, validate_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data loaders to be used in training and testing \n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(validate_x), torch.from_numpy(validate_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "#batch_size for dataloaders\n",
    "batch_size = 256\n",
    "\n",
    "num_worker = 0\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True, num_workers=num_worker)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True, num_workers=num_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the dataset are not very large let's try a few architecture, while doing a simple grid search for each architecture optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e375c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.01....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.099413... Val Loss: 1.098823\n",
      "Validation loss decreased (inf --> 1.098823).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.082270... Val Loss: 1.079725\n",
      "Validation loss decreased (1.098823 --> 1.079725).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.066272... Val Loss: 0.999108\n",
      "Validation loss decreased (1.079725 --> 0.999108).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.899566... Val Loss: 0.978857\n",
      "Validation loss decreased (0.999108 --> 0.978857).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.819070... Val Loss: 0.851041\n",
      "Validation loss decreased (0.978857 --> 0.851041).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.749189... Val Loss: 0.950992\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.735386... Val Loss: 0.985871\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.711263... Val Loss: 0.824801\n",
      "Validation loss decreased (0.851041 --> 0.824801).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.740210... Val Loss: 1.046706\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.757336... Val Loss: 0.892213\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.02....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.096081... Val Loss: 1.101376\n",
      "Validation loss decreased (inf --> 1.101376).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.099471... Val Loss: 1.097518\n",
      "Validation loss decreased (1.101376 --> 1.097518).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.098985... Val Loss: 1.081659\n",
      "Validation loss decreased (1.097518 --> 1.081659).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.030672... Val Loss: 1.038019\n",
      "Validation loss decreased (1.081659 --> 1.038019).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.023884... Val Loss: 1.041916\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.997827... Val Loss: 1.063025\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.080303... Val Loss: 1.049090\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.002092... Val Loss: 0.999336\n",
      "Validation loss decreased (1.038019 --> 0.999336).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.021090... Val Loss: 1.061168\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.943183... Val Loss: 1.019721\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.03....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.086177... Val Loss: 1.080533\n",
      "Validation loss decreased (inf --> 1.080533).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.075757... Val Loss: 1.088615\n",
      "Epoch: 3/10... Step: 936... Loss: 1.085974... Val Loss: 1.105201\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.098454... Val Loss: 1.101395\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.063051... Val Loss: 1.071853\n",
      "Validation loss decreased (1.080533 --> 1.071853).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.052006... Val Loss: 1.101116\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.063701... Val Loss: 1.091986\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.061880... Val Loss: 1.063797\n",
      "Validation loss decreased (1.071853 --> 1.063797).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.345883... Val Loss: 1.150951\n",
      "Epoch: 10/10... Step: 3120... Loss: nan... Val Loss: nan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.04....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.105891... Val Loss: 1.109773\n",
      "Validation loss decreased (inf --> 1.109773).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.111822... Val Loss: 1.104077\n",
      "Validation loss decreased (1.109773 --> 1.104077).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.122388... Val Loss: 1.116384\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.111147... Val Loss: 1.111794\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.112228... Val Loss: 1.115789\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.101270... Val Loss: 1.117156\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.112366... Val Loss: 1.099344\n",
      "Validation loss decreased (1.104077 --> 1.099344).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.102659... Val Loss: 1.106825\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.097130... Val Loss: 1.113218\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.122247... Val Loss: 1.119281\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.05....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.097911... Val Loss: 1.099778\n",
      "Validation loss decreased (inf --> 1.099778).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.141598... Val Loss: 1.102058\n",
      "Epoch: 3/10... Step: 936... Loss: 1.099620... Val Loss: 1.123470\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.099012... Val Loss: 1.104291\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.129187... Val Loss: 1.109502\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.100541... Val Loss: 1.099209\n",
      "Validation loss decreased (1.099778 --> 1.099209).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.096543... Val Loss: 1.118236\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.091576... Val Loss: 1.106666\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.098626... Val Loss: 1.102073\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.108780... Val Loss: 1.112668\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.06....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.157031... Val Loss: 1.135631\n",
      "Validation loss decreased (inf --> 1.135631).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.115141... Val Loss: 1.103285\n",
      "Validation loss decreased (1.135631 --> 1.103285).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.104915... Val Loss: 1.105384\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.106161... Val Loss: 1.147615\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.164862... Val Loss: 1.127863\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.110429... Val Loss: 1.102638\n",
      "Validation loss decreased (1.103285 --> 1.102638).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.113574... Val Loss: 1.125262\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.097874... Val Loss: 1.134844\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.169602... Val Loss: 1.116881\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.098790... Val Loss: 1.123681\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.07....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.144546... Val Loss: 1.173903\n",
      "Validation loss decreased (inf --> 1.173903).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.142470... Val Loss: 1.198416\n",
      "Epoch: 3/10... Step: 936... Loss: 1.143484... Val Loss: 1.147294\n",
      "Validation loss decreased (1.173903 --> 1.147294).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.098272... Val Loss: 1.113027\n",
      "Validation loss decreased (1.147294 --> 1.113027).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.155868... Val Loss: 1.109506\n",
      "Validation loss decreased (1.113027 --> 1.109506).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.107982... Val Loss: 1.108063\n",
      "Validation loss decreased (1.109506 --> 1.108063).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.169960... Val Loss: 1.102787\n",
      "Validation loss decreased (1.108063 --> 1.102787).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.116749... Val Loss: 1.142184\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.230552... Val Loss: 1.135206\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.107016... Val Loss: 1.175686\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.08....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.108884... Val Loss: 1.134182\n",
      "Validation loss decreased (inf --> 1.134182).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.197201... Val Loss: 1.110443\n",
      "Validation loss decreased (1.134182 --> 1.110443).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.231394... Val Loss: 1.112347\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.111874... Val Loss: 1.113423\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.218598... Val Loss: 1.233852\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.120346... Val Loss: 1.226059\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.115138... Val Loss: 1.131354\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.141280... Val Loss: 1.110359\n",
      "Validation loss decreased (1.110443 --> 1.110359).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.121537... Val Loss: 1.111268\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.129777... Val Loss: 1.101531\n",
      "Validation loss decreased (1.110359 --> 1.101531).  Saving model ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.09....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.134587... Val Loss: 1.119791\n",
      "Validation loss decreased (inf --> 1.119791).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.359004... Val Loss: 1.301546\n",
      "Epoch: 3/10... Step: 936... Loss: 1.179752... Val Loss: 1.107773\n",
      "Validation loss decreased (1.119791 --> 1.107773).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.121881... Val Loss: 1.197607\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.125822... Val Loss: 1.139033\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.115873... Val Loss: 1.114641\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.119371... Val Loss: 1.130028\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.100368... Val Loss: 1.123104\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.126480... Val Loss: 1.104074\n",
      "Validation loss decreased (1.107773 --> 1.104074).  Saving model ...\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.114897... Val Loss: 1.117835\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.001....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.099889... Val Loss: 1.098562\n",
      "Validation loss decreased (inf --> 1.098562).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.107444... Val Loss: 1.098493\n",
      "Validation loss decreased (1.098562 --> 1.098493).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.983790... Val Loss: 0.965053\n",
      "Validation loss decreased (1.098493 --> 0.965053).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.789569... Val Loss: 0.755687\n",
      "Validation loss decreased (0.965053 --> 0.755687).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.662983... Val Loss: 0.749605\n",
      "Validation loss decreased (0.755687 --> 0.749605).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.571472... Val Loss: 0.757554\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.491345... Val Loss: 0.814406\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.469947... Val Loss: 0.862615\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.435520... Val Loss: 0.956884\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.262958... Val Loss: 0.988422\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.002....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.100311... Val Loss: 1.099126\n",
      "Validation loss decreased (inf --> 1.099126).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.099072... Val Loss: 1.100682\n",
      "Epoch: 3/10... Step: 936... Loss: 1.081455... Val Loss: 1.107217\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.899260... Val Loss: 0.943328\n",
      "Validation loss decreased (1.099126 --> 0.943328).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.692214... Val Loss: 0.801996\n",
      "Validation loss decreased (0.943328 --> 0.801996).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.687288... Val Loss: 0.790750\n",
      "Validation loss decreased (0.801996 --> 0.790750).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.557338... Val Loss: 0.815626\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.594914... Val Loss: 0.871233\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.497845... Val Loss: 0.958055\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.305531... Val Loss: 1.036429\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.003....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.102956... Val Loss: 1.098735\n",
      "Validation loss decreased (inf --> 1.098735).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 0.980135... Val Loss: 0.961668\n",
      "Validation loss decreased (1.098735 --> 0.961668).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.756128... Val Loss: 0.857206\n",
      "Validation loss decreased (0.961668 --> 0.857206).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.741284... Val Loss: 0.760340\n",
      "Validation loss decreased (0.857206 --> 0.760340).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.581049... Val Loss: 0.748481\n",
      "Validation loss decreased (0.760340 --> 0.748481).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.667634... Val Loss: 0.776050\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.564768... Val Loss: 0.783748\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.518336... Val Loss: 0.823480\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.373838... Val Loss: 0.885345\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.370664... Val Loss: 0.936739\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.004....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.103529... Val Loss: 1.107904\n",
      "Validation loss decreased (inf --> 1.107904).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.025048... Val Loss: 1.023762\n",
      "Validation loss decreased (1.107904 --> 1.023762).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.835733... Val Loss: 0.875984\n",
      "Validation loss decreased (1.023762 --> 0.875984).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.763148... Val Loss: 0.792372\n",
      "Validation loss decreased (0.875984 --> 0.792372).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.808345... Val Loss: 0.773702\n",
      "Validation loss decreased (0.792372 --> 0.773702).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.695401... Val Loss: 0.766203\n",
      "Validation loss decreased (0.773702 --> 0.766203).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.624455... Val Loss: 0.779606\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.532964... Val Loss: 0.794757\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.545566... Val Loss: 0.793516\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.433531... Val Loss: 0.816884\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.005....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.098517... Val Loss: 1.098580\n",
      "Validation loss decreased (inf --> 1.098580).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.089054... Val Loss: 1.070746\n",
      "Validation loss decreased (1.098580 --> 1.070746).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.970405... Val Loss: 0.936708\n",
      "Validation loss decreased (1.070746 --> 0.936708).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.731299... Val Loss: 0.807640\n",
      "Validation loss decreased (0.936708 --> 0.807640).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.750622... Val Loss: 0.759773\n",
      "Validation loss decreased (0.807640 --> 0.759773).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.589995... Val Loss: 0.742709\n",
      "Validation loss decreased (0.759773 --> 0.742709).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.631146... Val Loss: 0.764733\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.617113... Val Loss: 0.763869\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.473956... Val Loss: 0.794332\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.527157... Val Loss: 0.795864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.006....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.097410... Val Loss: 1.099431\n",
      "Validation loss decreased (inf --> 1.099431).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.094106... Val Loss: 1.092591\n",
      "Validation loss decreased (1.099431 --> 1.092591).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.015445... Val Loss: 0.991066\n",
      "Validation loss decreased (1.092591 --> 0.991066).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.884854... Val Loss: 0.918972\n",
      "Validation loss decreased (0.991066 --> 0.918972).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.773168... Val Loss: 0.814702\n",
      "Validation loss decreased (0.918972 --> 0.814702).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.646885... Val Loss: 0.789926\n",
      "Validation loss decreased (0.814702 --> 0.789926).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.611259... Val Loss: 0.759214\n",
      "Validation loss decreased (0.789926 --> 0.759214).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.666704... Val Loss: 0.773660\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.562977... Val Loss: 0.770948\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.576791... Val Loss: 0.828145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.007....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.059296... Val Loss: 1.080432\n",
      "Validation loss decreased (inf --> 1.080432).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 0.990630... Val Loss: 0.952277\n",
      "Validation loss decreased (1.080432 --> 0.952277).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.839892... Val Loss: 0.868773\n",
      "Validation loss decreased (0.952277 --> 0.868773).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.781189... Val Loss: 0.886519\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.752646... Val Loss: 0.808433\n",
      "Validation loss decreased (0.868773 --> 0.808433).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.711224... Val Loss: 0.790824\n",
      "Validation loss decreased (0.808433 --> 0.790824).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.663132... Val Loss: 0.774208\n",
      "Validation loss decreased (0.790824 --> 0.774208).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.598562... Val Loss: 0.779365\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.567187... Val Loss: 0.806040\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.654628... Val Loss: 0.801545\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.008....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.097614... Val Loss: 1.099610\n",
      "Validation loss decreased (inf --> 1.099610).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 0.978874... Val Loss: 0.995642\n",
      "Validation loss decreased (1.099610 --> 0.995642).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.902235... Val Loss: 0.912236\n",
      "Validation loss decreased (0.995642 --> 0.912236).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.845779... Val Loss: 0.878374\n",
      "Validation loss decreased (0.912236 --> 0.878374).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.832773... Val Loss: 0.834351\n",
      "Validation loss decreased (0.878374 --> 0.834351).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.773352... Val Loss: 0.810701\n",
      "Validation loss decreased (0.834351 --> 0.810701).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.753586... Val Loss: 0.805249\n",
      "Validation loss decreased (0.810701 --> 0.805249).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.679530... Val Loss: 0.958534\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.625368... Val Loss: 0.845078\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.653794... Val Loss: 0.816351\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.009....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.100017... Val Loss: 1.098718\n",
      "Validation loss decreased (inf --> 1.098718).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.096338... Val Loss: 1.098755\n",
      "Epoch: 3/10... Step: 936... Loss: 0.921255... Val Loss: 0.919190\n",
      "Validation loss decreased (1.098718 --> 0.919190).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.775164... Val Loss: 0.884451\n",
      "Validation loss decreased (0.919190 --> 0.884451).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.767104... Val Loss: 1.072635\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.732035... Val Loss: 0.929222\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.700323... Val Loss: 1.062680\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.658124... Val Loss: 0.916765\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.633582... Val Loss: 0.900625\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.684938... Val Loss: 0.828362\n",
      "Validation loss decreased (0.884451 --> 0.828362).  Saving model ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_list = [\n",
    "    1e-2, 2e-2, 3e-2, 4e-2, 5e-2, 6e-2, 7e-2, 8e-2, 9e-2,\n",
    "    1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 6e-3, 7e-3, 8e-3, 9e-3\n",
    "]\n",
    "# lr_list = [\n",
    "#     1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6,\n",
    "# ]\n",
    "for lr in lr_list:\n",
    "    torch.cuda.empty_cache()    \n",
    "    #make sure the gpu is avalibale before moving the data to It else the work would be done using the cpu\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "    #specifing the model hypermetrs and creating the model\n",
    "    vocab_size = len(vocab_to_int)+1\n",
    "    output_size = 3\n",
    "    embedding_dim = 300\n",
    "    hidden_dim = 200\n",
    "    n_layers = 2\n",
    "    \n",
    "    net = multi_class_model.SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, train_on_gpu=train_on_gpu,\n",
    "                    bidirectional=False, drop_prob=0.5)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    # optimization functions\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=0.00001)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    print(f'training with learning rate of {lr}....')\n",
    "    # training the model\n",
    "    train(net, train_loader, valid_loader, optimizer, vocab_to_int, batch_size, multi_class=True, seq_length=256, epochs=10,\n",
    "        print_every=1, train_on_gpu=train_on_gpu, save_dic='models/arabic_01.pth')\n",
    "    print('\\n\\n\\n\\n')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()    \n",
    "# make sure the gpu is avalibale before moving the data to It else the work would be done using the cpu\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "# specifing the model hypermetrs and creating the model\n",
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 3\n",
    "embedding_dim = 300\n",
    "hidden_dim = 200\n",
    "n_layers = 2\n",
    "\n",
    "net = multi_class_model.SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, train_on_gpu=train_on_gpu,\n",
    "            bidirectional=False, drop_prob=0.5)\n",
    "\n",
    "print(net)\n",
    "\n",
    "lr = 5e-3\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=0.00001)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 312... Loss: 1.097829... Val Loss: 1.098659\n",
      "Validation loss decreased (inf --> 1.098659).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 0.954658... Val Loss: 0.994099\n",
      "Validation loss decreased (1.098659 --> 0.994099).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.904346... Val Loss: 0.869085\n",
      "Validation loss decreased (0.994099 --> 0.869085).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.980943... Val Loss: 0.964746\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.826960... Val Loss: 0.805186\n",
      "Validation loss decreased (0.869085 --> 0.805186).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.690083... Val Loss: 0.817309\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.638771... Val Loss: 0.796134\n",
      "Validation loss decreased (0.805186 --> 0.796134).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.708226... Val Loss: 0.773453\n",
      "Validation loss decreased (0.796134 --> 0.773453).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.626841... Val Loss: 0.800724\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.525634... Val Loss: 0.759343\n",
      "Validation loss decreased (0.773453 --> 0.759343).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "train(net, train_loader, valid_loader, optimizer, vocab_to_int, batch_size, multi_class=True, seq_length=256, epochs=10,\n",
    "print_every=1, train_on_gpu=train_on_gpu, save_dic='models/arabic_01.pth')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d587e422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.770\n",
      "Test accuracy: 0.650\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "test(test_loader, train_on_gpu, batch_size, multi_class=True, net=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.775\n",
      "Test accuracy: 0.646\n"
     ]
    }
   ],
   "source": [
    "# testing the last saved model\n",
    "test(test_loader, train_on_gpu, batch_size, multi_class=True, model_path='models/arabic_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.01....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.099513... Val Loss: 1.099207\n",
      "Validation loss decreased (inf --> 1.099207).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.056846... Val Loss: 1.046425\n",
      "Validation loss decreased (1.099207 --> 1.046425).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.963622... Val Loss: 0.955144\n",
      "Validation loss decreased (1.046425 --> 0.955144).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.832367... Val Loss: 0.953540\n",
      "Validation loss decreased (0.955144 --> 0.953540).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.880639... Val Loss: 0.929750\n",
      "Validation loss decreased (0.953540 --> 0.929750).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.835394... Val Loss: 0.940484\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.785608... Val Loss: 0.945341\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.841957... Val Loss: 0.942905\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.806352... Val Loss: 0.949982\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.838565... Val Loss: 0.939009\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.05....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.147095... Val Loss: 1.102463\n",
      "Validation loss decreased (inf --> 1.102463).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.109397... Val Loss: 1.119239\n",
      "Epoch: 3/10... Step: 936... Loss: 1.088182... Val Loss: 1.114983\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.102275... Val Loss: 1.129425\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.116187... Val Loss: 1.132746\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.088404... Val Loss: 1.115653\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.067189... Val Loss: 1.115284\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.103144... Val Loss: 1.121569\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.111883... Val Loss: 1.116552\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.079022... Val Loss: 1.108932\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.001....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.100096... Val Loss: 1.099730\n",
      "Validation loss decreased (inf --> 1.099730).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.093134... Val Loss: 1.099414\n",
      "Validation loss decreased (1.099730 --> 1.099414).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.088559... Val Loss: 1.103386\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.930634... Val Loss: 0.979976\n",
      "Validation loss decreased (1.099414 --> 0.979976).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.821850... Val Loss: 0.910227\n",
      "Validation loss decreased (0.979976 --> 0.910227).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.724134... Val Loss: 0.791364\n",
      "Validation loss decreased (0.910227 --> 0.791364).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.579307... Val Loss: 0.781552\n",
      "Validation loss decreased (0.791364 --> 0.781552).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.585951... Val Loss: 0.814576\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.488188... Val Loss: 0.883951\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.520902... Val Loss: 0.935370\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.005....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.097938... Val Loss: 1.099016\n",
      "Validation loss decreased (inf --> 1.099016).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 0.959457... Val Loss: 0.956187\n",
      "Validation loss decreased (1.099016 --> 0.956187).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 0.729588... Val Loss: 0.804301\n",
      "Validation loss decreased (0.956187 --> 0.804301).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.646317... Val Loss: 0.771706\n",
      "Validation loss decreased (0.804301 --> 0.771706).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.623263... Val Loss: 0.784255\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.586724... Val Loss: 0.798659\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.539325... Val Loss: 0.827915\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.604855... Val Loss: 0.909247\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.425716... Val Loss: 0.938588\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.358006... Val Loss: 0.988905\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.0001....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.100050... Val Loss: 1.100048\n",
      "Validation loss decreased (inf --> 1.100048).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.096656... Val Loss: 1.098691\n",
      "Validation loss decreased (1.100048 --> 1.098691).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.097110... Val Loss: 1.098602\n",
      "Validation loss decreased (1.098691 --> 1.098602).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.096533... Val Loss: 1.099119\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.100425... Val Loss: 1.098482\n",
      "Validation loss decreased (1.098602 --> 1.098482).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.101763... Val Loss: 1.098876\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.099602... Val Loss: 1.098684\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.090841... Val Loss: 1.098530\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.099371... Val Loss: 1.098499\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.093930... Val Loss: 1.098553\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 0.0005....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.103865... Val Loss: 1.100134\n",
      "Validation loss decreased (inf --> 1.100134).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.097034... Val Loss: 1.098576\n",
      "Validation loss decreased (1.100134 --> 1.098576).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.085482... Val Loss: 1.099076\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.088360... Val Loss: 1.101572\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.047336... Val Loss: 1.034954\n",
      "Validation loss decreased (1.098576 --> 1.034954).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.955482... Val Loss: 0.921504\n",
      "Validation loss decreased (1.034954 --> 0.921504).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.812488... Val Loss: 0.847069\n",
      "Validation loss decreased (0.921504 --> 0.847069).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.768399... Val Loss: 0.802158\n",
      "Validation loss decreased (0.847069 --> 0.802158).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.651883... Val Loss: 0.781990\n",
      "Validation loss decreased (0.802158 --> 0.781990).  Saving model ...\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.583237... Val Loss: 0.801013\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 1e-05....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.099916... Val Loss: 1.098779\n",
      "Validation loss decreased (inf --> 1.098779).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.100204... Val Loss: 1.098807\n",
      "Epoch: 3/10... Step: 936... Loss: 1.099511... Val Loss: 1.098801\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.100951... Val Loss: 1.098779\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.099652... Val Loss: 1.098762\n",
      "Validation loss decreased (1.098779 --> 1.098762).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.099031... Val Loss: 1.098778\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.099681... Val Loss: 1.098801\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.097968... Val Loss: 1.098695\n",
      "Validation loss decreased (1.098762 --> 1.098695).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.096934... Val Loss: 1.098678\n",
      "Validation loss decreased (1.098695 --> 1.098678).  Saving model ...\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.099906... Val Loss: 1.098795\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 5e-05....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.098491... Val Loss: 1.099016\n",
      "Validation loss decreased (inf --> 1.099016).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.098390... Val Loss: 1.098502\n",
      "Validation loss decreased (1.099016 --> 1.098502).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.097045... Val Loss: 1.098451\n",
      "Validation loss decreased (1.098502 --> 1.098451).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.100970... Val Loss: 1.098416\n",
      "Validation loss decreased (1.098451 --> 1.098416).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.101589... Val Loss: 1.099453\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.093840... Val Loss: 1.098637\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.098448... Val Loss: 1.098528\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.099008... Val Loss: 1.098451\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.097329... Val Loss: 1.098461\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.097242... Val Loss: 1.098697\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 200)\n",
      "  (lstm): LSTM(200, 150, batch_first=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc1): Linear(in_features=150, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "training with learning rate of 1e-06....\n",
      "Epoch: 1/10... Step: 312... Loss: 1.096206... Val Loss: 1.098608\n",
      "Validation loss decreased (inf --> 1.098608).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.099601... Val Loss: 1.098281\n",
      "Validation loss decreased (1.098608 --> 1.098281).  Saving model ...\n",
      "Epoch: 3/10... Step: 936... Loss: 1.098478... Val Loss: 1.098279\n",
      "Validation loss decreased (1.098281 --> 1.098279).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 1.100688... Val Loss: 1.098295\n",
      "Epoch: 5/10... Step: 1560... Loss: 1.100235... Val Loss: 1.098281\n",
      "Epoch: 6/10... Step: 1872... Loss: 1.098222... Val Loss: 1.098278\n",
      "Validation loss decreased (1.098279 --> 1.098278).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 1.098754... Val Loss: 1.098275\n",
      "Validation loss decreased (1.098278 --> 1.098275).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 1.099543... Val Loss: 1.098274\n",
      "Validation loss decreased (1.098275 --> 1.098274).  Saving model ...\n",
      "Epoch: 9/10... Step: 2808... Loss: 1.097899... Val Loss: 1.098276\n",
      "Epoch: 10/10... Step: 3120... Loss: 1.099239... Val Loss: 1.098270\n",
      "Validation loss decreased (1.098274 --> 1.098270).  Saving model ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr_list = [\n",
    "#     1e-2, 2e-2, 3e-2, 4e-2, 5e-2, 6e-2, 7e-2, 8e-2, 9e-2,\n",
    "#     1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 6e-3, 7e-3, 8e-3, 9e-3\n",
    "# ]\n",
    "lr_list = [\n",
    "    1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6,\n",
    "]\n",
    "for lr in lr_list:\n",
    "    torch.cuda.empty_cache()    \n",
    "    #make sure the gpu is avalibale before moving the data to It else the work would be done using the cpu\n",
    "    train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "    #specifing the model hypermetrs and creating the model\n",
    "    vocab_size = len(vocab_to_int)+1\n",
    "    output_size = 3\n",
    "    embedding_dim = 200\n",
    "    hidden_dim = 150\n",
    "    n_layers = 1\n",
    "    \n",
    "    net = multi_class_model.SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, train_on_gpu=train_on_gpu,\n",
    "                    bidirectional=False, drop_prob=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    # optimization functions\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=0.00001)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    print(f'training with learning rate of {lr}....')\n",
    "    # training the model\n",
    "    train(net, train_loader, valid_loader, optimizer, vocab_to_int, batch_size, multi_class=True, seq_length=256, epochs=10,\n",
    "        print_every=1, train_on_gpu=train_on_gpu, save_dic='models/arabic_01.pth')\n",
    "    print('\\n\\n\\n\\n')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embd): Embedding(35032, 300)\n",
      "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=400, out_features=3, bias=True)\n",
      "  (LogSoftmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()    \n",
    "# make sure the gpu is avalibale before moving the data to It else the work would be done using the cpu\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "# specifing the model hypermetrs and creating the model\n",
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 3\n",
    "embedding_dim = 300\n",
    "hidden_dim = 200\n",
    "n_layers = 2\n",
    "\n",
    "net = multi_class_model.SentimentRNN(\n",
    "    vocab_size, output_size, embedding_dim, hidden_dim, n_layers,\n",
    "    train_on_gpu=train_on_gpu, bidirectional=False, drop_prob=0.5\n",
    "    )\n",
    "\n",
    "print(net)\n",
    "\n",
    "lr = 5e-3\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=0.00001)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 312... Loss: 1.098610... Val Loss: 1.098869\n",
      "Validation loss decreased (inf --> 1.098869).  Saving model ...\n",
      "Epoch: 2/10... Step: 624... Loss: 1.097386... Val Loss: 1.099022\n",
      "Epoch: 3/10... Step: 936... Loss: 0.881335... Val Loss: 0.940504\n",
      "Validation loss decreased (1.098869 --> 0.940504).  Saving model ...\n",
      "Epoch: 4/10... Step: 1248... Loss: 0.870823... Val Loss: 0.830931\n",
      "Validation loss decreased (0.940504 --> 0.830931).  Saving model ...\n",
      "Epoch: 5/10... Step: 1560... Loss: 0.772145... Val Loss: 0.805867\n",
      "Validation loss decreased (0.830931 --> 0.805867).  Saving model ...\n",
      "Epoch: 6/10... Step: 1872... Loss: 0.686307... Val Loss: 0.763641\n",
      "Validation loss decreased (0.805867 --> 0.763641).  Saving model ...\n",
      "Epoch: 7/10... Step: 2184... Loss: 0.606457... Val Loss: 0.751648\n",
      "Validation loss decreased (0.763641 --> 0.751648).  Saving model ...\n",
      "Epoch: 8/10... Step: 2496... Loss: 0.612857... Val Loss: 0.763980\n",
      "Epoch: 9/10... Step: 2808... Loss: 0.499376... Val Loss: 0.770630\n",
      "Epoch: 10/10... Step: 3120... Loss: 0.528010... Val Loss: 0.777713\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "train(net, train_loader, valid_loader, optimizer, vocab_to_int, batch_size, multi_class=True, seq_length=256, epochs=10,\n",
    "print_every=1, train_on_gpu=train_on_gpu, save_dic='models/arabic_01.pth')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d587e422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.807\n",
      "Test accuracy: 0.651\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "test(test_loader, train_on_gpu, batch_size, multi_class=True, net=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.769\n",
      "Test accuracy: 0.650\n"
     ]
    }
   ],
   "source": [
    "# testing the last saved model\n",
    "test(test_loader, train_on_gpu, batch_size, multi_class=True, model_path='models/arabic_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ca738696e74c3b582319100ac25d026cf0c528d8decfe2c9d2d27159d6836ba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Graduation-Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
