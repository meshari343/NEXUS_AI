{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_maksura_ar, normalize_alef_ar, normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(filepath, outputpath):\n",
    "    # st = ISRIStemmer()\n",
    "    # Reading the data inside the xml\n",
    "    with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Passing the stored data inside\n",
    "    # the beautifulsoup parser\n",
    "    Bs_dataset = BeautifulSoup(data, \"xml\")\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    polarities = []\n",
    "    for sentence in Bs_dataset.find_all('sentence'):\n",
    "        drop = False\n",
    "        aspects = [x['target'] for x in sentence.find_all('Opinion') if x['target'] != 'NULL']\n",
    "        # print(aspects)  \n",
    "        # process the aspect so it can be used later on to create the target\n",
    "        for i, aspect in enumerate(aspects[:]):\n",
    "            aspects[i] = normalize_unicode(aspects[i])\n",
    "            # Normalizing alef variants to (ا)\n",
    "            aspects[i] = normalize_alef_ar(aspects[i])\n",
    "            # Normalizing alef maksura (ى) to yeh (ي)\n",
    "            aspects[i] = normalize_alef_maksura_ar(aspects[i])\n",
    "            # Normalizing teh marbuta (ة) to heh (ه)\n",
    "            aspects[i] = normalize_teh_marbuta_ar(aspects[i])\n",
    "            # removing Arabic diacritical marks\n",
    "            aspects[i] = dediac_ar(aspects[i])\n",
    "            # split each aspect\n",
    "            aspects[i] = simple_word_tokenize(aspects[i])  \n",
    "            # aspects[i] = [st.stem(word) for word in aspects[i]]\n",
    "\n",
    "        text = sentence.text\n",
    "        # process the text \n",
    "        text = normalize_unicode(text)\n",
    "        # Normalizing alef variants to (ا)\n",
    "        text = normalize_alef_ar(text)\n",
    "        # Normalizing alef maksura (ى) to yeh (ي)\n",
    "        text = normalize_alef_maksura_ar(text)\n",
    "        # Normalizing teh marbuta (ة) to heh (ه)\n",
    "        text = normalize_teh_marbuta_ar(text)\n",
    "        # removing Arabic diacritical marks\n",
    "        text = dediac_ar(text)\n",
    "        # split the text \n",
    "        text_split = simple_word_tokenize(text)\n",
    "\n",
    "        # text_split = [st.stem(word) for word in text_split]\n",
    "        \n",
    "        # create target list where the start of the aspect is 1, the inside is 2, and non aspects are 0\n",
    "        row_tags = np.zeros((len(text_split),), dtype=np.int16)\n",
    "        for aspect in aspects:\n",
    "            # if the aspct contain multiaple words \n",
    "            for i, word in enumerate(aspect):\n",
    "                try:\n",
    "                    # assign one for the start of the aspect\n",
    "                    if i == 0:\n",
    "                        row_tags[text_split.index(word)] = 1\n",
    "                    # assign 2 for the remaining words of the aspect\n",
    "                    else:\n",
    "                        row_tags[text_split.index(word)] = 2\n",
    "                except ValueError:\n",
    "                    drop = True\n",
    "                    # print(word)\n",
    "        if not drop:\n",
    "            sentiments = [x['polarity'] for x in sentence.find_all('Opinion') if x['target'] != 'NULL']\n",
    "            # create target list where the start of the aspect is 1, the inside is 2, and non aspects are 0\n",
    "            row_polarities = np.empty((len(text_split),), dtype=np.int16)\n",
    "            row_polarities[:] = -1\n",
    "            for aspect, aspect_sentiment in zip(aspects, sentiments):\n",
    "                # if the aspct contain multiaple words \n",
    "                for i, word in enumerate(aspect):\n",
    "                    try:\n",
    "                        if aspect_sentiment == 'positive':\n",
    "                            row_polarities[text_split.index(word)] = 2\n",
    "                        elif aspect_sentiment == 'neutral':\n",
    "                            row_polarities[text_split.index(word)] = 1\n",
    "                        elif aspect_sentiment == 'negative':\n",
    "                            row_polarities[text_split.index(word)] = 0\n",
    "                    except ValueError:\n",
    "                        print('unexpected error')\n",
    "                                     \n",
    "            tokens.append(text_split)\n",
    "            tags.append(row_tags.flatten().tolist())\n",
    "            polarities.append(row_polarities.flatten().tolist())\n",
    "\n",
    "\n",
    "    dict = {'Tokens': tokens, 'Tags': tags, 'Polarities': polarities}     \n",
    "    df = pd.DataFrame(dict) \n",
    "    # print(len(tokens))\n",
    "    # print(len(tags))\n",
    "    # print(len(polarities))\n",
    "    # print(tokens[:3])\n",
    "    # print(tokens[:3])\n",
    "    # print(rows[:3])\n",
    "    # print(text_split)\n",
    "    # print(aspects)\n",
    "    # print(target)\n",
    "    # saving the dataframe \n",
    "    df.to_csv(outputpath, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv('data/Arabic_Hotels_TrD_V2.xml', 'dataset/arabic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('dataset/arabic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3603, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tokens  \\\n",
       "0  ['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...   \n",
       "1  ['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...   \n",
       "2  ['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...   \n",
       "3  ['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...   \n",
       "4  ['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2               [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3      [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]   \n",
       "4         [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          Polarities  \n",
       "0  [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...  \n",
       "1  [2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...  \n",
       "2    [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]  \n",
       "3  [-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...  \n",
       "4  [2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv('data/AR_HOTE_SB1_TEST.xml.gold', 'dataset/arabic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv('dataset/arabic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3603, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tokens  \\\n",
       "0  ['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...   \n",
       "1  ['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...   \n",
       "2  ['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...   \n",
       "3  ['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...   \n",
       "4  ['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2               [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3      [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]   \n",
       "4         [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          Polarities  \n",
       "0  [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...  \n",
       "1  [2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...  \n",
       "2    [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]  \n",
       "3  [-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...  \n",
       "4  [2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testing_data.shape)\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like there's a problem in the data the aspects sometimes doesn't match the text for example an aspect could be والفندق but in the text it's الفندق and many other diffrences, even after stemming it got down from 300 to 100 but there's still some spelling and other possible mistakes.\n",
    "\n",
    "to ensure the data quality, and avoid any possible mistake in the data, any row with an error would be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1d4d6eb2abe3fb4049954bd206a32ef86aac931501ef1b113439c6e56c2b586"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
