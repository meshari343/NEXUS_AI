{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_maksura_ar, normalize_alef_ar, normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_maksura_ar, normalize_alef_ar, normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import csv\n",
    "\n",
    "def write_json_IOB(texts, texts_aspects, outputpath, normalize=True):\n",
    "    '''\n",
    "    texts: a list of input texts\n",
    "    texts_aspects: a list containing lists of aspects for each input text\n",
    "    outputpath: path to save the data\n",
    "    normalize: boolean flag to indicate whatever to normalize input arabic text\n",
    "    '''\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for i in range(len(texts)):\n",
    "        drop = False\n",
    "        aspects = texts_aspects[i]\n",
    "        text = texts[i]\n",
    "        # process the aspect so it can be used later on to create the target\n",
    "        if normalize:\n",
    "            for j, aspect in enumerate(aspects[:]):\n",
    "                aspects[j] = normalize_unicode(aspects[j])\n",
    "                # Normalizing alef variants to (ا)\n",
    "                aspects[j] = normalize_alef_ar(aspects[j])\n",
    "                # Normalizing alef maksura (ى) to yeh (ي)\n",
    "                aspects[j] = normalize_alef_maksura_ar(aspects[j])\n",
    "                # Normalizing teh marbuta (ة) to heh (ه)\n",
    "                aspects[j] = normalize_teh_marbuta_ar(aspects[j])\n",
    "                # removing Arabic diacritical marks\n",
    "                aspects[j] = dediac_ar(aspects[j])\n",
    "                # aspects[j] = [st.stem(word) for word in aspects[j]]\n",
    "            # normalize input text\n",
    "            text = normalize_unicode(text)\n",
    "            # Normalizing alef variants to (ا)\n",
    "            text = normalize_alef_ar(text)\n",
    "            # Normalizing alef maksura (ى) to yeh (ي)\n",
    "            text = normalize_alef_maksura_ar(text)\n",
    "            # Normalizing teh marbuta (ة) to heh (ه)\n",
    "            text = normalize_teh_marbuta_ar(text)\n",
    "            # removing Arabic diacritical marks\n",
    "            text = dediac_ar(text)\n",
    "\n",
    "        # split each aspect\n",
    "        for j, aspect in enumerate(aspects[:]):\n",
    "            aspects[j] = simple_word_tokenize(aspects[j])  \n",
    "        # split the text \n",
    "        text_split = simple_word_tokenize(text)\n",
    "        # text_split = [st.stem(word) for word in text_split]\n",
    "        \n",
    "        # create target list where the start of the aspect is 1, the inside is 2, and non aspects are 0\n",
    "        row_tags = np.zeros((len(text_split),), dtype=np.int16)\n",
    "        for aspect in aspects:\n",
    "            # assgin tags for the aspect\n",
    "            for i, word in enumerate(aspect):\n",
    "                try:\n",
    "                    # assign one for the start of the aspect\n",
    "                    if i == 0:\n",
    "                        row_tags[text_split.index(word)] = 1\n",
    "                    # assign 2 for the remaining words of the aspect\n",
    "                    else:\n",
    "                        row_tags[text_split.index(word)] = 2\n",
    "                except ValueError:\n",
    "                    drop = True\n",
    "        if not drop:                         \n",
    "            tokens.append(text_split)\n",
    "            tags.append(row_tags.flatten().tolist())\n",
    "\n",
    "\n",
    "    dict = {'Tokens': tokens, 'Tags': tags}     \n",
    "    df = pd.DataFrame(dict) \n",
    "\n",
    "    df.to_json(outputpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[تجربة, الان]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[تجربة, للمستقبل]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tokens    Tags\n",
       "0      [تجربة, الان]  [1, 0]\n",
       "1  [تجربة, للمستقبل]  [1, 2]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IOB_to_json import write_json_IOB\n",
    "write_json_IOB(['تجربة الان', 'تجربة للمستقبل'], [['تجربة'], ['تجربة للمستقبل']], 'dataset/arabic_train.json', normalize=False)\n",
    "pd.read_json('dataset/arabic_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('dataset/arabic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3603, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tokens  \\\n",
       "0  ['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...   \n",
       "1  ['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...   \n",
       "2  ['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...   \n",
       "3  ['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...   \n",
       "4  ['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2               [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3      [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]   \n",
       "4         [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          Polarities  \n",
       "0  [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...  \n",
       "1  [2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...  \n",
       "2    [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]  \n",
       "3  [-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...  \n",
       "4  [2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv('data/AR_HOTE_SB1_TEST.xml.gold', 'dataset/arabic_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv('dataset/arabic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3603, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...</td>\n",
       "      <td>[0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]</td>\n",
       "      <td>[-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tokens  \\\n",
       "0  ['انصح', 'بالنوم', 'وليس', 'تناول', 'الطعام', ...   \n",
       "1  ['فندق', 'يتميز', 'بمرافق', 'نوعيه', 'وخلاقه',...   \n",
       "2  ['لمسه', 'بحريه', 'جميل', 'وظريفه', '،', 'فندق...   \n",
       "3  ['ساوصي', 'بالتاكيد', 'بموقع', 'المدينه', 'الق...   \n",
       "4  ['فريق', 'العمل', 'الودود', 'والمتعاون', 'علي'...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0            [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2               [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3      [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]   \n",
       "4         [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          Polarities  \n",
       "0  [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1...  \n",
       "1  [2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1...  \n",
       "2    [-1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1]  \n",
       "3  [-1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 2, 2, -...  \n",
       "4  [2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testing_data.shape)\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like there's a problem in the data the aspects sometimes doesn't match the text for example an aspect could be والفندق but in the text it's الفندق and many other diffrences, even after stemming it got down from 300 to 100 but there's still some spelling and other possible mistakes.\n",
    "\n",
    "to ensure the data quality, and avoid any possible mistake in the data, any row with an error would be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1d4d6eb2abe3fb4049954bd206a32ef86aac931501ef1b113439c6e56c2b586"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
