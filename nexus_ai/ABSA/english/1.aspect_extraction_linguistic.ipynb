{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba467034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meshari\\anaconda3\\envs\\Graduation-Project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\meshari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2574a5",
   "metadata": {},
   "source": [
    "<img src=\"files/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95fdd4",
   "metadata": {},
   "source": [
    "we can see from the picture below the structure of the data, what we are intrested in is the target which are the aspcet terms to compare with the extracted aspcet using the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6f2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data inside the xml\n",
    "with open('EN_REST_SB1_TEST.xml.gold', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# Passing the stored data inside\n",
    "# the beautifulsoup parser\n",
    "Bs_data = BeautifulSoup(data, \"lxml\")\n",
    " \n",
    "# Finding all instances of tag `opinion`\n",
    "opinions = Bs_data.find_all('opinions')\n",
    "# extracting the last opinion for a sentence \n",
    "# and getting the targets (aspects) values for the that opinion\n",
    "aspects = [x.find_all(\"opinion\")[-1][\"target\"] if len(x) > 0 else None for x in opinions]\n",
    "# getting polarity (y values) \n",
    "y_test = [x.find_all(\"opinion\")[-1][\"polarity\"] if len(x) > 0 else None for x in opinions if len(x) > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f4c3dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['NULL'],\n",
       " ['sushi'],\n",
       " ['portions'],\n",
       " ['Green Tea creme brulee'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['place'],\n",
       " ['sushi', 'service'],\n",
       " ['staff'],\n",
       " ['restaurant'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['owner'],\n",
       " ['food'],\n",
       " [],\n",
       " ['meal'],\n",
       " ['NULL'],\n",
       " ['staff'],\n",
       " ['onion rings'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['food'],\n",
       " ['lemon chicken', 'honey walnut prawns', 'honey walnut prawns'],\n",
       " [],\n",
       " ['ambience', 'place'],\n",
       " ['NULL'],\n",
       " ['Service'],\n",
       " [],\n",
       " ['service'],\n",
       " ['waiter'],\n",
       " ['manager'],\n",
       " ['NULL'],\n",
       " ['Italian Food'],\n",
       " ['Mioposto'],\n",
       " ['Italian restaurant'],\n",
       " ['wine list', 'food'],\n",
       " ['restaurant'],\n",
       " ['meal', 'service', 'ambiance'],\n",
       " ['NULL'],\n",
       " ['wine list', 'food', 'staff'],\n",
       " ['place'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " ['food'],\n",
       " ['food', 'food'],\n",
       " ['Kafta plate'],\n",
       " ['NULL'],\n",
       " ['meal'],\n",
       " ['food', 'service'],\n",
       " ['place', 'food'],\n",
       " ['food'],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['atmosphere', 'food', 'food'],\n",
       " ['food'],\n",
       " ['Rice', 'tuna'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['fish', 'fish'],\n",
       " ['service', 'NULL'],\n",
       " [],\n",
       " ['fish', 'rolls'],\n",
       " ['service'],\n",
       " ['waiter', 'staff members'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['sushi', 'restaurant'],\n",
       " ['miso soup', 'rolls'],\n",
       " ['sashimi'],\n",
       " ['apps'],\n",
       " ['place', 'portions'],\n",
       " ['space', 'service'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['food', 'waiters'],\n",
       " ['NULL'],\n",
       " ['service'],\n",
       " ['NULL', 'NULL'],\n",
       " ['service'],\n",
       " ['bus boy'],\n",
       " ['food', 'salsa'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['food', 'margaritas', 'waitress'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['pizza place'],\n",
       " ['pizza'],\n",
       " ['NULL'],\n",
       " ['Sushi'],\n",
       " [],\n",
       " ['NULL', 'NULL'],\n",
       " ['NULL', 'NULL'],\n",
       " ['calamari'],\n",
       " ['deck', 'NULL'],\n",
       " ['place'],\n",
       " ['music', 'staff', 'NULL'],\n",
       " ['bar'],\n",
       " ['NULL'],\n",
       " ['bartender'],\n",
       " ['NULL'],\n",
       " ['pizza', 'draft and bottle selection'],\n",
       " [\"jukebox's\"],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['food', 'bartenders'],\n",
       " ['NULL'],\n",
       " ['owners', 'beer selection'],\n",
       " ['upstairs'],\n",
       " ['balconey'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " ['sushi'],\n",
       " ['fish'],\n",
       " ['Jellyfish', 'Horse Mackerel', 'Blue Fin Tuna', 'Sake Ikura roll'],\n",
       " ['pieces'],\n",
       " ['decor'],\n",
       " ['crowd'],\n",
       " ['service'],\n",
       " ['food'],\n",
       " [],\n",
       " ['menu'],\n",
       " [],\n",
       " ['food', 'food'],\n",
       " ['food', 'place'],\n",
       " ['Service'],\n",
       " ['Drinks'],\n",
       " ['drinks', 'NULL'],\n",
       " ['NULL'],\n",
       " ['food', 'ambience', 'NULL'],\n",
       " ['pumpkin ravioli',\n",
       "  'goat cheese gnocchi',\n",
       "  'goat cheese gnocchi',\n",
       "  'filet mignon on top of spinach and mashed potatoes'],\n",
       " ['ambiance'],\n",
       " ['NULL', 'artwork', 'music'],\n",
       " ['NULL', 'NULL'],\n",
       " ['Indian food'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['chicken curry', 'chicken tikka masala'],\n",
       " ['chana masala (garbanzo beans)'],\n",
       " ['location', 'food'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['service', 'NULL'],\n",
       " ['menu'],\n",
       " ['saag', 'paneer', 'korma'],\n",
       " ['NULL'],\n",
       " ['delivery'],\n",
       " ['food', 'food'],\n",
       " ['meal'],\n",
       " ['lamb korma', 'saag paneer', 'samosas', 'naan'],\n",
       " ['food', 'food'],\n",
       " ['NULL'],\n",
       " ['food'],\n",
       " [],\n",
       " ['naan', 'bhartha'],\n",
       " ['chickpeas'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['fajita salad', 'colorado', 'fajitas', 'NULL', 'place'],\n",
       " ['environment'],\n",
       " ['place'],\n",
       " ['restaurant', 'decor', 'customer service', 'manager'],\n",
       " [],\n",
       " [],\n",
       " ['food'],\n",
       " ['atmosphere'],\n",
       " [],\n",
       " ['lunch spot'],\n",
       " ['mexican spot'],\n",
       " ['service', 'NULL'],\n",
       " ['taco salads', 'burritos', 'enchiladas', 'place', 'NULL'],\n",
       " ['margaritas'],\n",
       " ['NULL'],\n",
       " ['Lebanese Food'],\n",
       " ['Open Sesame'],\n",
       " ['food'],\n",
       " [],\n",
       " ['food', 'food'],\n",
       " ['chicken shawarma'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['staff', 'decor'],\n",
       " ['OPEN SESAME'],\n",
       " ['Hummus'],\n",
       " ['food', 'lunch'],\n",
       " ['Dinners'],\n",
       " ['open sesame combo plate', 'open sesame combo plate'],\n",
       " ['side of potatoes', 'labne (yogurt dip)'],\n",
       " ['hummus', 'hummus'],\n",
       " ['seating', 'service'],\n",
       " ['NULL'],\n",
       " ['place'],\n",
       " ['beer'],\n",
       " ['restaurant'],\n",
       " ['NULL'],\n",
       " ['beer', 'meal'],\n",
       " ['NULL'],\n",
       " ['band', 'service'],\n",
       " [],\n",
       " ['shoe string onions',\n",
       "  'goat cheese pizza',\n",
       "  'grilled asparagus',\n",
       "  'fried brie with fruit'],\n",
       " ['NULL'],\n",
       " ['beer', 'NULL'],\n",
       " ['waiter'],\n",
       " ['band'],\n",
       " ['Winelist'],\n",
       " ['wine list'],\n",
       " ['sommelier'],\n",
       " ['wine'],\n",
       " ['NULL'],\n",
       " ['PLACE'],\n",
       " ['place'],\n",
       " ['food', 'potions', 'service', 'food'],\n",
       " ['owners'],\n",
       " ['establishment'],\n",
       " ['Scooner or Later'],\n",
       " ['NULL'],\n",
       " ['Scooner or Later'],\n",
       " ['NULL'],\n",
       " ['Place', 'Pizza', 'Coffee'],\n",
       " ['pizza menu', 'pizza menu'],\n",
       " ['coffe'],\n",
       " ['ambience'],\n",
       " ['place'],\n",
       " ['servers behind the counter'],\n",
       " [],\n",
       " ['food', 'place'],\n",
       " ['NULL'],\n",
       " [\"Ray's\"],\n",
       " ['food', 'location', 'service'],\n",
       " ['food'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['Food', 'Food'],\n",
       " ['kafta plate'],\n",
       " ['NULL', 'NULL'],\n",
       " ['Atmosphere'],\n",
       " ['Service', 'waitress'],\n",
       " ['place'],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['FOOD', 'PEOPLE', 'NULL'],\n",
       " ['food', 'atmosphere'],\n",
       " ['Eggs', 'pancakes', 'potatoes', 'fresh fruit', 'yogurt', 'NULL'],\n",
       " ['place'],\n",
       " ['meal', 'fish on the omikase platter'],\n",
       " ['waitstaff'],\n",
       " ['waiter', 'sake'],\n",
       " ['NULL'],\n",
       " ['sushi'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['nigiri', 'sashmi', 'rolls'],\n",
       " ['oyster roll'],\n",
       " ['NULL'],\n",
       " ['sushi'],\n",
       " ['restaurant'],\n",
       " ['Chuwam Mushi'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['Sushi', 'Sushi'],\n",
       " ['Sushi place'],\n",
       " ['house special roll'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['spot'],\n",
       " [\"Murphy's\"],\n",
       " ['NULL'],\n",
       " ['Brunch', 'food', 'egg white omelet'],\n",
       " ['spot', 'spot'],\n",
       " ['desserts'],\n",
       " ['wine list', 'ambiance'],\n",
       " ['NULL', 'NULL'],\n",
       " ['NULL'],\n",
       " ['Mexican place'],\n",
       " ['enchiladas', 'chicken soup', 'specials'],\n",
       " ['cooks'],\n",
       " ['service'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['sushi', 'wait staff'],\n",
       " ['NULL'],\n",
       " ['atmosphere'],\n",
       " ['Space', 'food'],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['staff'],\n",
       " ['employees'],\n",
       " ['staff'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['Caesar salad'],\n",
       " [],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['food', 'Margaritas'],\n",
       " ['NULL', 'portions', 'service', 'margaritas'],\n",
       " ['service'],\n",
       " ['art on the walls'],\n",
       " ['place'],\n",
       " ['meal'],\n",
       " ['indain food'],\n",
       " ['lunch menu'],\n",
       " ['food'],\n",
       " ['ingrediants', 'NULL'],\n",
       " ['NULL'],\n",
       " ['food', 'service'],\n",
       " ['fish', 'service'],\n",
       " ['selection', 'sushi'],\n",
       " ['waiters'],\n",
       " ['NULL'],\n",
       " ['place'],\n",
       " ['brunch food', 'NULL'],\n",
       " ['regular menu', 'NULL'],\n",
       " ['NULL'],\n",
       " ['sushi',\n",
       "  'fish',\n",
       "  'sake',\n",
       "  'SOHO location',\n",
       "  'Salmon',\n",
       "  'Tuna',\n",
       "  'Fluke',\n",
       "  'Yellow Tail',\n",
       "  'Cod',\n",
       "  'Mackeral',\n",
       "  'Jellyfish',\n",
       "  'Sea Urchin',\n",
       "  'Shrimp',\n",
       "  'Lobster',\n",
       "  'Sea Bream',\n",
       "  'Trout',\n",
       "  'Milk Fish',\n",
       "  'Blue Fin Tuna',\n",
       "  'Eel',\n",
       "  'Crab',\n",
       "  'Sardine',\n",
       "  'Monk Fish',\n",
       "  'Roe',\n",
       "  'Scallop',\n",
       "  'Oysters',\n",
       "  'Toro'],\n",
       " ['place'],\n",
       " [],\n",
       " ['BLUE RIBBON SUSHI'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL', 'NULL'],\n",
       " ['tuna'],\n",
       " ['Blue Ribbon'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['sushi', 'sushi', 'setting'],\n",
       " ['NULL'],\n",
       " ['rolls', 'smoked yellowtail', \"Chef's Choice for sushi\"],\n",
       " ['customer service', 'pizza'],\n",
       " ['service', 'food'],\n",
       " ['pizza'],\n",
       " ['pizza'],\n",
       " [],\n",
       " ['location', 'Mioposto'],\n",
       " ['NULL'],\n",
       " ['place'],\n",
       " [],\n",
       " ['food', 'waiting staff'],\n",
       " ['wait'],\n",
       " ['NULL'],\n",
       " ['Seafood'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['appetizer of oysters, lobster, crab (small size)'],\n",
       " ['Seabass on lobster risotto'],\n",
       " ['Caesar salad'],\n",
       " ['bottle of wine'],\n",
       " ['dessert'],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " ['food', 'service'],\n",
       " ['NULL'],\n",
       " ['waitress'],\n",
       " ['owner'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['PLACE'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['atmosphere', 'service'],\n",
       " ['service'],\n",
       " ['NULL'],\n",
       " ['restaurant'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['bar keep'],\n",
       " ['service'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['Crab Cakes'],\n",
       " ['cakes'],\n",
       " ['place'],\n",
       " ['seasonal fish', 'seafood', 'waterfront setting'],\n",
       " ['NULL'],\n",
       " ['Pizza', 'Service'],\n",
       " ['pizza', 'mushroom pizza'],\n",
       " ['caeser salad'],\n",
       " ['servers', 'young woman'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['waitstaff', 'Management'],\n",
       " ['pizza', 'service'],\n",
       " ['place', 'service'],\n",
       " ['Breakfast'],\n",
       " ['breakfast'],\n",
       " ['NULL'],\n",
       " ['food', 'bloody mary'],\n",
       " ['breakfast'],\n",
       " ['crab eggs benedict'],\n",
       " ['menu items', 'NULL'],\n",
       " ['NULL'],\n",
       " ['chinese food'],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['ambiance'],\n",
       " ['NULL'],\n",
       " ['staff'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['prawns', 'batter', 'walnuts'],\n",
       " ['NULL'],\n",
       " ['honey walnyt prawns'],\n",
       " [],\n",
       " ['brocollis'],\n",
       " ['scallops', 'prawns', 'brocolli'],\n",
       " ['NULL'],\n",
       " ['mango ginger creme brulee'],\n",
       " ['NULL'],\n",
       " ['restaurant'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['portions'],\n",
       " ['sushi', 'restaurant', 'sushi', 'sushi'],\n",
       " ['NULL', 'NULL'],\n",
       " ['restaurant'],\n",
       " ['service', 'food', 'NULL'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['service'],\n",
       " ['Food', 'restaurant', 'restaurant'],\n",
       " [],\n",
       " ['location'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['feel', 'glass walls'],\n",
       " [],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['Spanish Mackeral special appetizer', 'box sushi', 'eel with avodcao'],\n",
       " ['omikase', 'omikase'],\n",
       " ['specialty rolls'],\n",
       " ['selection of sakes'],\n",
       " ['Green tea creme brulee'],\n",
       " ['sushi', 'sushi'],\n",
       " ['customer service'],\n",
       " ['location along the marina in Long Beach', 'food', 'customer service'],\n",
       " ['food', 'restaurant staff'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['waitress'],\n",
       " ['manager'],\n",
       " ['manager'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " [],\n",
       " ['sushi'],\n",
       " ['fish', 'fish'],\n",
       " ['service'],\n",
       " ['sake selection'],\n",
       " ['Dungeness crabs'],\n",
       " ['seafood dinners'],\n",
       " ['NULL', 'NULL'],\n",
       " ['Rays'],\n",
       " ['Dungeness crabs'],\n",
       " ['black cod in sake kasu'],\n",
       " ['dessert of a port and chocolate tasting'],\n",
       " ['service'],\n",
       " ['Breakfast'],\n",
       " ['drinks', 'corn beef hash', 'coffee', 'B Fast burritos', 'menu'],\n",
       " ['service', 'place'],\n",
       " ['place'],\n",
       " ['NULL'],\n",
       " ['Standby'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['spot'],\n",
       " ['music', 'bar tenders', 'atmosphere'],\n",
       " ['NULL'],\n",
       " ['cheese fries'],\n",
       " ['Food', 'Service', 'NULL'],\n",
       " ['price fixed pre-show dinner'],\n",
       " ['wood decor'],\n",
       " ['music', 'subwoofer to the sound system'],\n",
       " [],\n",
       " ['shellfish and shrimp appetizer'],\n",
       " ['seafood', 'seafood'],\n",
       " [],\n",
       " ['asparagus'],\n",
       " [],\n",
       " ['9 oz steak'],\n",
       " ['steak'],\n",
       " ['peppercorn sauce', 'red wine reduction'],\n",
       " ['steak'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['desert', 'dinner'],\n",
       " ['NULL', 'NULL'],\n",
       " ['servers'],\n",
       " ['restaurant'],\n",
       " ['NULL'],\n",
       " ['sake', 'food'],\n",
       " [],\n",
       " ['sake', 'Server'],\n",
       " ['lobster 3 ways'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['waiter', 'feel'],\n",
       " ['bathroom stall'],\n",
       " ['menu'],\n",
       " ['food', 'food'],\n",
       " ['food', 'food'],\n",
       " ['NULL'],\n",
       " ['pizza joint'],\n",
       " ['restaurant'],\n",
       " ['NULL'],\n",
       " ['pizza', 'salads'],\n",
       " ['wait staff', 'cooks'],\n",
       " ['NULL'],\n",
       " ['decor', 'feeling'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['Mioposto CafÃ©'],\n",
       " ['pizzaâ€™s'],\n",
       " ['menu'],\n",
       " ['menu', 'pizzaâ€™s'],\n",
       " ['NULL'],\n",
       " ['pizzaâ€™s'],\n",
       " ['Pizza Ensalata'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['pizzaâ€™s', 'crust'],\n",
       " ['NULL'],\n",
       " ['NULL', 'NULL'],\n",
       " ['food'],\n",
       " ['staff'],\n",
       " ['atmosphere'],\n",
       " ['pancakes'],\n",
       " ['menu'],\n",
       " ['breakfast', 'NULL'],\n",
       " ['eats'],\n",
       " [],\n",
       " [],\n",
       " ['place'],\n",
       " ['place'],\n",
       " ['fish'],\n",
       " ['chef special dinner', 'chef special dinner'],\n",
       " ['plate', 'exotic fish', 'exotic fish'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['atmosphere'],\n",
       " ['honwy walnut prawns'],\n",
       " ['service'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['service', 'coffee', 'food'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['restaurant'],\n",
       " ['food', 'food'],\n",
       " ['decor'],\n",
       " ['music', 'atmosphere', 'vibe', 'people'],\n",
       " ['server'],\n",
       " ['Open Sesame'],\n",
       " ['NULL'],\n",
       " ['place'],\n",
       " ['NULL'],\n",
       " ['menu'],\n",
       " ['waiters'],\n",
       " ['sushi'],\n",
       " ['NULL', 'portions', 'sushi dishes'],\n",
       " ['pepperoni'],\n",
       " [\"pepperoni's\"],\n",
       " ['pizza'],\n",
       " ['NULL'],\n",
       " ['NULL'],\n",
       " ['Balcony'],\n",
       " ['NULL'],\n",
       " ['Food'],\n",
       " ['Portions'],\n",
       " ['menu selection'],\n",
       " ['appetizer', 'Creme Brulee'],\n",
       " ['Indoor ambience'],\n",
       " ['balcony'],\n",
       " ['place'],\n",
       " ['service', 'NULL'],\n",
       " ['server', 'NULL'],\n",
       " ['NULL'],\n",
       " ['food'],\n",
       " [],\n",
       " [],\n",
       " ['NULL', 'table'],\n",
       " ['service', 'waiters'],\n",
       " ['food', 'filet mignon'],\n",
       " ['portions'],\n",
       " ['music'],\n",
       " ['NULL'],\n",
       " ['restroom'],\n",
       " ['bathroom'],\n",
       " [],\n",
       " ['NULL'],\n",
       " ['place'],\n",
       " ['service', 'NULL'],\n",
       " ['chips  and salsa', 'NULL'],\n",
       " ['atmosphere', 'decor'],\n",
       " ['Raouls chicken vegetable soup', 'catering'],\n",
       " ['Drinks', 'NULL'],\n",
       " ['NULL'],\n",
       " ['Service'],\n",
       " [],\n",
       " ['Snooze', 'menu', 'Snooze'],\n",
       " ['orange juice'],\n",
       " ['juice', 'juice'],\n",
       " ['menu'],\n",
       " ['pancakes', 'pancakes', 'pancakes'],\n",
       " ['pancakes', 'pancakes'],\n",
       " [],\n",
       " ['eggs', 'meal'],\n",
       " ['dish'],\n",
       " ['manager', 'NULL'],\n",
       " ['pancakes', 'pancakes', 'breakfast burrito', 'orange juice', 'iced tea'],\n",
       " [],\n",
       " [],\n",
       " ['view', 'NULL'],\n",
       " [\"Ray's Boathouse\"],\n",
       " ['Sound views'],\n",
       " ['views', \"Ray's\"],\n",
       " ['Brooke'],\n",
       " ['white gulf prawns',\n",
       "  'manila clams',\n",
       "  'soy dressing',\n",
       "  'butter sauce',\n",
       "  'Shilshole Sampler',\n",
       "  'Shilshole Sampler'],\n",
       " ['seared Alaskan sea scallops', 'seared Alaskan sea scallops'],\n",
       " ['scallops', 'scallops'],\n",
       " ['grilled Alaskan King Salmon',\n",
       "  'creamed Washington russet potatoes',\n",
       "  'green beans'],\n",
       " ['plate'],\n",
       " ['server', \"Ray's\"],\n",
       " ['NULL'],\n",
       " [],\n",
       " [\"Ray's Boathouse\"],\n",
       " ['server', 'food', 'NULL'],\n",
       " ['view']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects = [x.find_all(\"opinion\") if len(x) > 0 else None for x in opinions]\n",
    "print(len(aspects))\n",
    "aspects_ = []\n",
    "for aspect in aspects:\n",
    "    if aspect:\n",
    "        aspect_ = []\n",
    "        for asp in aspect:\n",
    "            aspect_.append(asp['target'])\n",
    "        aspects_.append(aspect_)\n",
    "    else:\n",
    "        aspects_.append([])\n",
    "aspects_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f4794e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<opinion category=\"FOOD#QUALITY\" from=\"0\" polarity=\"positive\" target=\"NULL\" to=\"0\"></opinion>],\n",
       " [<opinion category=\"FOOD#QUALITY\" from=\"19\" polarity=\"positive\" target=\"sushi\" to=\"24\"></opinion>],\n",
       " [<opinion category=\"FOOD#STYLE_OPTIONS\" from=\"16\" polarity=\"neutral\" target=\"portions\" to=\"24\"></opinion>],\n",
       " [<opinion category=\"FOOD#QUALITY\" from=\"0\" polarity=\"positive\" target=\"Green Tea creme brulee\" to=\"22\"></opinion>],\n",
       " [<opinion category=\"FOOD#QUALITY\" from=\"0\" polarity=\"positive\" target=\"NULL\" to=\"0\"></opinion>],\n",
       " [<opinion category=\"RESTAURANT#GENERAL\" from=\"0\" polarity=\"positive\" target=\"NULL\" to=\"0\"></opinion>],\n",
       " [<opinion category=\"RESTAURANT#GENERAL\" from=\"32\" polarity=\"positive\" target=\"place\" to=\"37\"></opinion>],\n",
       " [<opinion category=\"FOOD#QUALITY\" from=\"13\" polarity=\"positive\" target=\"sushi\" to=\"18\"></opinion>,\n",
       "  <opinion category=\"SERVICE#GENERAL\" from=\"35\" polarity=\"positive\" target=\"service\" to=\"42\"></opinion>],\n",
       " [<opinion category=\"SERVICE#GENERAL\" from=\"11\" polarity=\"positive\" target=\"staff\" to=\"16\"></opinion>],\n",
       " [<opinion category=\"RESTAURANT#GENERAL\" from=\"18\" polarity=\"positive\" target=\"restaurant\" to=\"28\"></opinion>]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f974ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspects = Bs_data.find_all('opinion')\n",
    "# # aspects = [x[\"target\"] for x in aspects]\n",
    "# print(aspects[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161c44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_json('google_balanced.json')\n",
    "# data = google_clean_reviews(data)\n",
    "# reviews = list(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e8a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yum!\n"
     ]
    }
   ],
   "source": [
    "# gettin all text tags\n",
    "reviews = Bs_data.find_all('text', text=True)\n",
    "# extracting only the text from the text tags\n",
    "reviews = [x.text for x in reviews]\n",
    "# aspects = [x[\"target\"] for x in aspects]\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291b31bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48998799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2271a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[set(), set(), set(), {'brulee'}, set(), set(), set(), set(), {'staff'}, set()]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    target = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "            target.append(token.text)\n",
    "    pred.append(set(target))\n",
    "print(pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b4667",
   "metadata": {},
   "source": [
    "### testing the result without the testing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8f0ba79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct aspects: 157\n",
      "extra incorrect aspects: 187\n",
      "empty extracted aspects: 410\n"
     ]
    }
   ],
   "source": [
    "overall_correct = 0\n",
    "pred_list = []\n",
    "for x in pred:\n",
    "    pred_list.extend(list(x))\n",
    "len_pred = len(pred_list)\n",
    "\n",
    "for i in range(len(aspects_)):\n",
    "    review_correct = len(pred[i].intersection(aspects_[i]))\n",
    "    overall_correct += review_correct\n",
    "\n",
    "print(f'correct aspects: {overall_correct}')    \n",
    "print(f'extra incorrect aspects: {len_pred-overall_correct}')\n",
    "print(f'empty extracted aspects: {pred.count(set())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c95d1",
   "metadata": {},
   "source": [
    "### testing the result with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddbda404",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0] = pred[0].replace('', 'NULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fd980",
   "metadata": {},
   "source": [
    "in over all level it seems there is a lot of the aspcets being empty and we can see that from the exracted sample with 8 from the first 10 aspects being empty, with the test results without converting the empty values to null and thus not counting them we can see that the algorithm got only around 100 from a total of close to 700, and when take empty values into considration the number doubles, since most of the aspects wasn't caught in the first place with this strict algorithm, and that's because of the use of dependnce tag of nsubj, although it limits getting out of bound aspects but it also miss a lot of possible aspcects as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d5b3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null values = 420\n",
      "number of non null values = 256\n",
      "precentage of correctly extracted aspects =  39.45%\n",
      "precentage of correctly extracted aspects without accounting for null values =  38.61%\n"
     ]
    }
   ],
   "source": [
    "null_values = len(pred[pred.iloc[:,0]=='NULL'])\n",
    "non_null = len(pred) - null_values\n",
    "\n",
    "print(f'number of null values = {null_values}')\n",
    "print(f'number of non null values = {non_null}')\n",
    "print(f'precentage of correctly extracted aspects = {correct_no_null / non_null * 100 : .2f}%')\n",
    "print(f'precentage of correctly extracted aspects without accounting for null values = {correct_null / len(pred) * 100 : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b59b1",
   "metadata": {},
   "source": [
    "indeed this algorithm is too strict 419 out of the 676 is empty, which amounts to around 75% of the data, so let's see what's the results without using the nsubj dependnce tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2eee6f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[set(), {'sushi'}, {'portions'}, {'creme', 'must'}, {'restaurant'}, {'Comparison'}, {'place'}, {'service', 'sushi'}, {'staff', 'need'}, {'dozen', 'date', 'restaurant', 'times', 'complaints'}]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    target = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            target.append(token.text)\n",
    "    pred.append(set(target))\n",
    "print(pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "03450a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct aspects: 398\n",
      "extra incorrect aspects: 1295\n",
      "empty extracted aspects: 85\n"
     ]
    }
   ],
   "source": [
    "overall_correct = 0\n",
    "pred_list = []\n",
    "for x in pred:\n",
    "    pred_list.extend(list(x))\n",
    "len_pred = len(pred_list)\n",
    "\n",
    "for i in range(len(aspects_)):\n",
    "    review_correct = len(pred[i].intersection(aspects_[i]))\n",
    "    overall_correct += review_correct\n",
    "\n",
    "print(f'correct aspects: {overall_correct}')    \n",
    "print(f'extra incorrect aspects: {len_pred-overall_correct}')\n",
    "print(f'empty extracted aspects: {pred.count(set())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0811d",
   "metadata": {},
   "source": [
    "### testing the result without the counting null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a7624a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d208bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pred == aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d702d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    525\n",
       "True     151\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be935c",
   "metadata": {},
   "source": [
    "wee can see that the results improved from 101 to 151 without using the dependnce tag of nsubj, but we need to take into consideration the loose standred of this algorithm since it takes any noun as an aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8347d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the correct values to be used later\n",
    "correct_no_null = r.value_counts()[True].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82190c",
   "metadata": {},
   "source": [
    "### testing the result with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75a77283",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0] = pred[0].replace('', 'NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2713a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.iloc[0] == aspects.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e067ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pred == aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "798d6c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    467\n",
       "True     209\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c050a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the correct values to be used later\n",
    "correct_null = r.value_counts()[True].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78fa3476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null values = 85\n",
      "number of non null values = 591\n",
      "accuracy of aspect extraction algorithm  =  35.36%\n",
      "accuracy of aspect extraction algorithm without accounting for null values =  30.92%\n"
     ]
    }
   ],
   "source": [
    "null_values = len(pred[pred.iloc[:,0]=='NULL'])\n",
    "non_null = len(pred) - null_values\n",
    "\n",
    "print(f'number of null values = {null_values}')\n",
    "print(f'number of non null values = {non_null}')\n",
    "print(f'accuracy of aspect extraction algorithm  = {correct_no_null / non_null * 100 : .2f}%')\n",
    "print(f'accuracy of aspect extraction algorithm without accounting for null values = {correct_null / len(pred) * 100 : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2f3ed",
   "metadata": {},
   "source": [
    "only 85 values is with empty aspects, but as a drawback out of the extracted aspects only 35.36% of them was correct, compared to using nsubj dependnce tag which give 39.45% with an adavantage of , and an even bigger diffrence when we take into account the null value wa can see that the use of the msubj dependnce tag have a around 8% of an advantage, finally we can see that using nsubj dependnce tag gives better quality.\n",
    "\n",
    "in the end using nsubj dependnce tag, give more precise results but with more missing aspects, in the other hand without using nsubj dependnce tag, it gives a higher number of non empty aspects with a lower accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bb1f9",
   "metadata": {},
   "source": [
    "as a final note: the noun taking into consideration is the last found noun, if it's possible to extract the right noun it should give a better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551a732",
   "metadata": {},
   "source": [
    "Now I would try a few linguistic rules presented in a research paper containing 3 sets of rules two implemented previously and the last one is one he proposed so let's try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "44111f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    target = []\n",
    "    for i in range(len(doc)):\n",
    "        if i != 0:\n",
    "            word1 = doc[i-1]\n",
    "        else:\n",
    "            word1 = nlp(' ')[0]\n",
    "        word2 = doc[i]\n",
    "        if i+1 != len(doc):\n",
    "            word3 = doc[i+1]\n",
    "        else:\n",
    "            word3 = nlp(' ')[0]\n",
    "            \n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS'):\n",
    "            target.append(word1.text)\n",
    "                \n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "            \n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ') and \\\n",
    "           (word3.tag_ != 'NN' or word3.tag_ != 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'JJ') and \\\n",
    "           (word3.tag_ != 'NN' or word3.tag_ != 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word3.text)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS') and \\\n",
    "           (word2.tag_ == 'JJ') and \\\n",
    "           (word3.tag_ != 'NN' or word3.tag_ != 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word3.text)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'VB' or word2.tag_ == 'VBD' or word2.tag_ == 'VBN' or word2.tag_ == 'VBG'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "            \n",
    "    pred.append(set(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c990ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct aspects: 406\n",
      "extra incorrect aspects: 2163\n",
      "empty extracted aspects: 71\n",
      "unique aspects : 1480\n"
     ]
    }
   ],
   "source": [
    "overall_correct = 0\n",
    "pred_list = []\n",
    "for x in pred:\n",
    "    pred_list.extend(list(x))\n",
    "len_pred = len(pred_list)\n",
    "\n",
    "for i in range(len(aspects_)):\n",
    "    review_correct = len(pred[i].intersection(aspects_[i]))\n",
    "    overall_correct += review_correct\n",
    "\n",
    "print(f'correct aspects: {overall_correct}')    \n",
    "print(f'extra incorrect aspects: {len_pred-overall_correct}')\n",
    "print(f'empty extracted aspects: {pred.count(set())}')\n",
    "print(f'unique aspects : {len(set(pred_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "7bd9c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    target = []\n",
    "    for i in range(len(doc)):\n",
    "        if i != 0:\n",
    "            word1 = doc[i-1]\n",
    "        else:\n",
    "            word1 = nlp(' ')[0]\n",
    "        word2 = doc[i]\n",
    "        if i+1 != len(doc):\n",
    "            word3 = doc[i+1]\n",
    "        else:\n",
    "            word3 = nlp(' ')[0]\n",
    " \n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS'):\n",
    "            target.append(word1.text)\n",
    "            \n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ' or word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'VBN' or word2.tag_ == 'VBD'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS') and \\\n",
    "           (word3.tag_ == 'JJ'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'VBN' or word1.tag_ == 'VBD') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'VBN' or word1.tag_ == 'VBD') and \\\n",
    "           (word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "            \n",
    "    pred.append(set(target))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "28e7f8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct aspects: 406\n",
      "extra incorrect aspects: 2163\n",
      "empty extracted aspects: 71\n"
     ]
    }
   ],
   "source": [
    "overall_correct = 0\n",
    "pred_list = []\n",
    "for x in pred:\n",
    "    pred_list.extend(list(x))\n",
    "len_pred = len(pred_list)\n",
    "\n",
    "for i in range(len(aspects_)):\n",
    "    review_correct = len(pred[i].intersection(aspects_[i]))\n",
    "    overall_correct += review_correct\n",
    "\n",
    "print(f'correct aspects: {overall_correct}')    \n",
    "print(f'extra incorrect aspects: {len_pred-overall_correct}')\n",
    "print(f'empty extracted aspects: {pred.count(set())}')\n",
    "print(f'unique aspects : {len(set(pred_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8095fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    target = []\n",
    "    for i in range(len(doc)):\n",
    "        if i != 0:\n",
    "            word1 = doc[i-1]\n",
    "        else:\n",
    "            word1 = nlp(' ')[0]\n",
    "        word2 = doc[i]\n",
    "        if i+1 != len(doc):\n",
    "            word3 = doc[i+1]\n",
    "        else:\n",
    "            word3 = nlp(' ')[0]\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS'):\n",
    "            target.append(word1.text)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'VB'):\n",
    "            target.append(word1.text)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'DT' or word1.tag_ == 'JJ' or word1.tag_ == 'NN') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS' or word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ' or word2.tag_ == 'VBN' or word2.tag_ == 'VBD'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'VBN' or word1.tag_ == 'VBD') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS' or word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ' or word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS' or word3.tag_ == 'JJ'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'VB' or word2.tag_ == 'VBN' or word2.tag_ == 'VBD') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS') and \\\n",
    "           (word2.tag_ == 'VB' or word2.tag_ == 'IN' or word2.tag_ == 'NN' or word2.tag_ == 'NNS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS') and \\\n",
    "           (word2.tag_ == 'IN') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN') and \\\n",
    "           (word2.tag_ == 'TO') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "            \n",
    "    pred.append(set(target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "9bbf81b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct aspects: 470\n",
      "extra incorrect aspects: 3486\n",
      "empty extracted aspects: 49\n",
      "unique aspects : 2373\n",
      "Precision: 0.905587668593449\n",
      "Precision: 0.11880687563195147\n",
      "unique correct aspects : 203\n"
     ]
    }
   ],
   "source": [
    "overall_correct = 0\n",
    "pred_list = []\n",
    "for x in pred:\n",
    "    pred_list.extend(list(x))\n",
    "len_pred = len(pred_list)\n",
    "\n",
    "for i in range(len(aspects_)):\n",
    "    review_correct = len(pred[i].intersection(aspects_[i]))\n",
    "    overall_correct += review_correct\n",
    "\n",
    "print(f'correct aspects: {overall_correct}')    \n",
    "print(f'extra incorrect aspects: {len_pred-overall_correct}')\n",
    "print(f'empty extracted aspects: {pred.count(set())}')\n",
    "print(f'unique aspects : {len(set(pred_list))}')\n",
    "print(f'Precision: {overall_correct/(overall_correct+(len_pred-overall_correct))}')\n",
    "aspect_list = []\n",
    "for x in aspects_:\n",
    "    aspect_list.extend(list(x))\n",
    "    \n",
    "print(f'unique correct aspects : {len(set(pred_list).intersection(aspect_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f9a830ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    target = []\n",
    "    for i in range(len(doc)):\n",
    "        if i != 0:\n",
    "            word1 = doc[i-1]\n",
    "        else:\n",
    "            word1 = nlp(' ')[0]\n",
    "        word2 = doc[i]\n",
    "        if i+1 != len(doc):\n",
    "            word3 = doc[i+1]\n",
    "        else:\n",
    "            word3 = nlp(' ')[0]\n",
    "            \n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS'):\n",
    "            target.append(word1.text)\n",
    "                \n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "            \n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ') and \\\n",
    "           (word3.tag_ != 'NN' or word3.tag_ != 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'JJ') and \\\n",
    "           (word3.tag_ != 'NN' or word3.tag_ != 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word3.text)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS') and \\\n",
    "           (word2.tag_ == 'JJ') and \\\n",
    "           (word3.tag_ != 'NN' or word3.tag_ != 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word3.text)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'VB' or word2.tag_ == 'VBD' or word2.tag_ == 'VBN' or word2.tag_ == 'VBG'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ' or word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'VBN' or word2.tag_ == 'VBD'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS') and \\\n",
    "           (word3.tag_ == 'JJ'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'VBN' or word1.tag_ == 'VBD') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "        if (word1.tag_ == 'VBN' or word1.tag_ == 'VBD') and \\\n",
    "           (word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "        if (word1.tag_ == 'VB'):\n",
    "            target.append(word1.text)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'DT' or word1.tag_ == 'JJ' or word1.tag_ == 'NN') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS' or word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ' or word2.tag_ == 'VBN' or word2.tag_ == 'VBD'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'VBN' or word1.tag_ == 'VBD') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS' or word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS'):\n",
    "            word = word1.text+' '+word2.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'NN' or word2.tag_ == 'NNS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'RB' or word1.tag_ == 'RBR' or word1.tag_ == 'RBS') and \\\n",
    "           (word2.tag_ == 'JJ' or word2.tag_ == 'RB' or word2.tag_ == 'RBR' or word2.tag_ == 'RBS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS' or word3.tag_ == 'JJ'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'JJ') and \\\n",
    "           (word2.tag_ == 'VB' or word2.tag_ == 'VBN' or word2.tag_ == 'VBD') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS') and \\\n",
    "           (word2.tag_ == 'VB' or word2.tag_ == 'IN' or word2.tag_ == 'NN' or word2.tag_ == 'NNS') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN' or word1.tag_ == 'NNS') and \\\n",
    "           (word2.tag_ == 'IN') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "        if (word1.tag_ == 'NN') and \\\n",
    "           (word2.tag_ == 'TO') and \\\n",
    "           (word3.tag_ == 'NN' or word3.tag_ == 'NNS'):\n",
    "            word = word1.text+' '+word2.text+' '+word3.text\n",
    "            target.append(word)            \n",
    "            \n",
    "    pred.append(set(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "d13fe225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct aspects: 471\n",
      "extra incorrect aspects: 3736\n",
      "empty extracted aspects: 47\n",
      "unique aspects : 2594\n",
      "Precision: 0.11195626337057285\n",
      "unique correct aspects : 204\n"
     ]
    }
   ],
   "source": [
    "overall_correct = 0\n",
    "pred_list = []\n",
    "for x in pred:\n",
    "    pred_list.extend(list(x))\n",
    "len_pred = len(pred_list)\n",
    "\n",
    "for i in range(len(aspects_)):\n",
    "    review_correct = len(pred[i].intersection(aspects_[i]))\n",
    "    overall_correct += review_correct\n",
    "\n",
    "print(f'correct aspects: {overall_correct}')    \n",
    "print(f'extra incorrect aspects: {len_pred-overall_correct}')\n",
    "print(f'empty extracted aspects: {pred.count(set())}')\n",
    "print(f'unique aspects : {len(set(pred_list))}')\n",
    "print(f'Precision: {overall_correct/(overall_correct+(len_pred-overall_correct))}')\n",
    "aspect_list = []\n",
    "for x in aspects_:\n",
    "    aspect_list.extend(list(x))\n",
    "    \n",
    "print(f'unique correct aspects : {len(set(pred_list).intersection(aspect_list))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fa0c7",
   "metadata": {},
   "source": [
    "After looking at the research papers on the first two rules, which were quoted by the research paper that I saw, it seems like this linguistic approach needs a filtering approach. The one implemented in the research papers discussing these linguistic rules used PMI as a filtering method. This method depends on a search engine, which I don't see as very effective. The best results in recent years seem to have been using supervised BERT models, and there's even an LCF-BERT model designed to provide optimized ABSA results that I would try next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
